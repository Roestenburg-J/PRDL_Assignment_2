{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQArg6UZpK6Q"
      },
      "source": [
        "This notebook is a copy of basic_nn.ipynb but with 2 convolution layers added to the \"LSTM\" model which is now a Convolution-LSTM model.\n",
        "Accuracy is still 50%\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTjtRYkxL447",
        "outputId": "e8a55d0e-effd-4a85-e870-7efb3dae9049"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No env_vars file found. Using default values. Make a \"env_vars.py\" file to change them.\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Locally get vars to accommodate different environments\n",
        "try:\n",
        "    from env_vars import *\n",
        "except:\n",
        "    print(\n",
        "        'No env_vars file found. Using default values. Make a \"env_vars.py\" file to change them.'\n",
        "    )\n",
        "    intra_train_path = \"./data/Intra/train/\"\n",
        "    intra_test_path = \"./data/Intra/test/\"\n",
        "\n",
        "    cross_train_path = \"./data/Cross/train/\"\n",
        "    cross_test1_path = \"./data/Cross/test1/\"\n",
        "    cross_test2_path = \"./data/Cross/test2/\"\n",
        "    cross_test3_path = \"./data/Cross/test3/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLJacqmIQHq3",
        "outputId": "0d56faab-e4ad-4855-9722-3ad941364676"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "\n",
        "# drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "5ltmaALlL449"
      },
      "outputs": [],
      "source": [
        "54  # Used to get the name of the dataset, and by extension, the label of the action being performed\n",
        "\n",
        "\n",
        "def getdatasetname(file_name_with_dir):\n",
        "    filename_without_dir = file_name_with_dir.split(\"/\")[-1]\n",
        "\n",
        "\n",
        "    temp = filename_without_dir.split(\"_\")[:-1]\n",
        "\n",
        "\n",
        "    datasetname = \"_\".join(temp)\n",
        "\n",
        "\n",
        "    return datasetname"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "rk_y7Ak6L449"
      },
      "outputs": [],
      "source": [
        "# Used to get the labels of data using filenames\n",
        "def get_label(filename):\n",
        "    if \"rest\" in filename:\n",
        "        label = 0\n",
        "\n",
        "    elif \"math\" in filename:\n",
        "        label = 1\n",
        "\n",
        "    elif \"memory\" in filename:\n",
        "        label = 2\n",
        "    elif \"motor\" in filename:\n",
        "        label = 3\n",
        "\n",
        "    return label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "CqLPp7xKL44_"
      },
      "outputs": [],
      "source": [
        "def get_all_matrices(dir_path):\n",
        "    dataset = []\n",
        "    labels = []\n",
        "    for filename in os.listdir(dir_path):\n",
        "        if filename.endswith(\".h5\"):\n",
        "            filename_path = dir_path + filename\n",
        "            with h5py.File(filename_path, \"r\") as f:\n",
        "                dataset_name = getdatasetname(filename_path)\n",
        "                label = get_label(filename)\n",
        "                matrix = f.get(dataset_name)[()]\n",
        "                dataset.append(matrix)\n",
        "                labels.append(label)\n",
        "\n",
        "    return dataset, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIJ5FFzTDNLS"
      },
      "source": [
        "MinMax Scaling for sensor data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "x0KQT9KkL45A"
      },
      "outputs": [],
      "source": [
        "def scale(matrix):\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "    scaler.fit(matrix)\n",
        "\n",
        "    scaled_data = scaler.transform(matrix)\n",
        "\n",
        "    return scaled_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0fH5C7pDNLT"
      },
      "source": [
        "Downsampling of data, refer to file called \"Notebook.ipynb\" to see how it works\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "UbSawgmiL45A"
      },
      "outputs": [],
      "source": [
        "def downsample(dataset, frequency):\n",
        "    downsampled_dataset = []\n",
        "\n",
        "    for i in range(0, dataset.shape[1], 2034):\n",
        "        second = dataset[:, i : i + 2034]\n",
        "        subsample = []\n",
        "\n",
        "        for j in range(0, 2034, int(2034 / frequency)):\n",
        "            if j < second.shape[1]:\n",
        "                measurement = second[:, j]\n",
        "                subsample.append(measurement)\n",
        "\n",
        "        downsampled_dataset.extend(subsample)\n",
        "\n",
        "    return np.array(downsampled_dataset).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYd4BFYML45D"
      },
      "source": [
        "Model setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-L0I5MdL45E",
        "outputId": "7f33f181-16a3-4033-8c77-511a19d53f0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.15.0\n"
          ]
        }
      ],
      "source": [
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTdGQvf9DNLT"
      },
      "source": [
        "This is where the archtiecture is specified. For now it is just a Recurrent Neural Network (RNN) with two layers with 128 parameters each i.e., not very sophisticated.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "N6LJwavfYyf7"
      },
      "outputs": [],
      "source": [
        "def load_data(dir_path):\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    for filename in os.listdir(dir_path):\n",
        "        if filename.endswith(\".h5\"):\n",
        "            filename_path = os.path.join(dir_path, filename)\n",
        "            with h5py.File(filename_path, \"r\") as f:\n",
        "                dataset_name = getdatasetname(filename_path)\n",
        "                label = get_label(filename)\n",
        "                matrix = f.get(dataset_name)[()]\n",
        "\n",
        "                train_meg = downsample(matrix, 113)\n",
        "                train_meg = scale(train_meg)\n",
        "\n",
        "                ############\n",
        "                # meshes_list = []\n",
        "                # arr = train_meg\n",
        "\n",
        "                # # Iterate through the original array in chunks\n",
        "                # for sensors in train_meg.T:\n",
        "                #     sensors = np.reshape(sensors, (1, 248))\n",
        "                #     mesh = array_to_mesh(sensors)\n",
        "\n",
        "                #     # Append the resulting array to the list\n",
        "                #     meshes_list.append(mesh)\n",
        "\n",
        "                # meshes_list = np.array(meshes_list)\n",
        "                # train_meg = meshes_list\n",
        "                # print(train_meg.shape)\n",
        "                ##############\n",
        "\n",
        "                flattened_meg = np.array(\n",
        "                    train_meg.flatten()\n",
        "                )  # The data is flattened to change the meg data from shape 248 x frequency to a 1D array (result: a lot of input params)\n",
        "\n",
        "                data.append(flattened_meg)\n",
        "                labels.append(label)\n",
        "\n",
        "    return np.array(data), np.array(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pui4NkPXPA_e"
      },
      "source": [
        "This function maps the 248 sensors to a 2D mesh. Taken from https://github.com/SMehrkanoon/Deep-brain-state-classification-of-MEG-data/blob/master/AA-CascadeNet_AA-MultiviewNet/data_utils.py#L369\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "3Q24Uw5KOxnn"
      },
      "outputs": [],
      "source": [
        "def array_to_mesh(arr):\n",
        "    input_rows = 20\n",
        "    input_columns = 21\n",
        "    input_channels = 248\n",
        "\n",
        "    assert arr.shape == (1, input_channels), (\n",
        "        \"the shape of the input array should be (1,248) because there are 248 MEG channels,received array of shape \"\n",
        "        + str(arr.shape)\n",
        "    )\n",
        "    output = np.zeros((input_rows, input_columns), dtype=float)\n",
        "\n",
        "    # 121\n",
        "    output[0][10] = arr[0][120]\n",
        "\n",
        "    # 89\n",
        "    output[1][12] = arr[0][151]\n",
        "    output[1][11] = arr[0][119]\n",
        "    output[1][10] = arr[0][88]\n",
        "    output[1][9] = arr[0][89]\n",
        "    output[1][8] = arr[0][121]\n",
        "\n",
        "    # 61\n",
        "    output[2][13] = arr[0][150]\n",
        "    output[2][12] = arr[0][118]\n",
        "    output[2][11] = arr[0][87]\n",
        "    output[2][10] = arr[0][60]\n",
        "    output[2][9] = arr[0][61]\n",
        "    output[2][8] = arr[0][90]\n",
        "    output[2][7] = arr[0][122]\n",
        "\n",
        "    # 37\n",
        "    output[3][14] = arr[0][149]\n",
        "    output[3][13] = arr[0][117]\n",
        "    output[3][12] = arr[0][86]\n",
        "    output[3][11] = arr[0][59]\n",
        "    output[3][10] = arr[0][36]\n",
        "    output[3][9] = arr[0][37]\n",
        "    output[3][8] = arr[0][62]\n",
        "    output[3][7] = arr[0][91]\n",
        "    output[3][6] = arr[0][123]\n",
        "\n",
        "    # 19\n",
        "    output[4][17] = arr[0][194]\n",
        "    output[4][16] = arr[0][175]\n",
        "    output[4][15] = arr[0][148]\n",
        "    output[4][14] = arr[0][116]\n",
        "    output[4][13] = arr[0][85]\n",
        "    output[4][12] = arr[0][58]\n",
        "    output[4][11] = arr[0][35]\n",
        "    output[4][10] = arr[0][18]\n",
        "    output[4][9] = arr[0][19]\n",
        "    output[4][8] = arr[0][38]\n",
        "    output[4][7] = arr[0][63]\n",
        "    output[4][6] = arr[0][92]\n",
        "    output[4][5] = arr[0][152]\n",
        "    output[4][4] = arr[0][176]\n",
        "\n",
        "    # 5\n",
        "    output[5][20] = arr[0][247]\n",
        "    output[5][19] = arr[0][227]\n",
        "    output[5][18] = arr[0][193]\n",
        "    output[5][17] = arr[0][174]\n",
        "    output[5][16] = arr[0][147]\n",
        "    output[5][15] = arr[0][115]\n",
        "    output[5][14] = arr[0][84]\n",
        "    output[5][13] = arr[0][57]\n",
        "    output[5][12] = arr[0][34]\n",
        "    output[5][11] = arr[0][17]\n",
        "    output[5][10] = arr[0][4]\n",
        "    output[5][9] = arr[0][5]\n",
        "    output[5][8] = arr[0][20]\n",
        "    output[5][7] = arr[0][39]\n",
        "    output[5][6] = arr[0][64]\n",
        "    output[5][5] = arr[0][93]\n",
        "    output[5][4] = arr[0][125]\n",
        "    output[5][3] = arr[0][153]\n",
        "    output[5][2] = arr[0][177]\n",
        "    output[5][1] = arr[0][211]\n",
        "    output[5][0] = arr[0][228]\n",
        "\n",
        "    # 4\n",
        "    output[6][20] = arr[0][246]\n",
        "    output[6][19] = arr[0][226]\n",
        "    output[6][18] = arr[0][192]\n",
        "    output[6][17] = arr[0][173]\n",
        "    output[6][16] = arr[0][146]\n",
        "    output[6][15] = arr[0][114]\n",
        "    output[6][14] = arr[0][83]\n",
        "    output[6][13] = arr[0][56]\n",
        "    output[6][12] = arr[0][33]\n",
        "    output[6][11] = arr[0][16]\n",
        "    output[6][10] = arr[0][3]\n",
        "    output[6][9] = arr[0][6]\n",
        "    output[6][8] = arr[0][21]\n",
        "    output[6][7] = arr[0][40]\n",
        "    output[6][6] = arr[0][65]\n",
        "    output[6][5] = arr[0][94]\n",
        "    output[6][4] = arr[0][126]\n",
        "    output[6][3] = arr[0][154]\n",
        "    output[6][2] = arr[0][178]\n",
        "    output[6][1] = arr[0][212]\n",
        "    output[6][0] = arr[0][229]\n",
        "\n",
        "    # 3\n",
        "    output[7][19] = arr[0][245]\n",
        "    output[7][18] = arr[0][210]\n",
        "    output[7][17] = arr[0][172]\n",
        "    output[7][16] = arr[0][145]\n",
        "    output[7][15] = arr[0][113]\n",
        "    output[7][14] = arr[0][82]\n",
        "    output[7][13] = arr[0][55]\n",
        "    output[7][12] = arr[0][32]\n",
        "    output[7][11] = arr[0][15]\n",
        "    output[7][10] = arr[0][2]\n",
        "    output[7][9] = arr[0][7]\n",
        "    output[7][8] = arr[0][22]\n",
        "    output[7][7] = arr[0][41]\n",
        "    output[7][6] = arr[0][66]\n",
        "    output[7][5] = arr[0][95]\n",
        "    output[7][4] = arr[0][127]\n",
        "    output[7][3] = arr[0][155]\n",
        "    output[7][2] = arr[0][195]\n",
        "    output[7][1] = arr[0][230]\n",
        "\n",
        "    # 8\n",
        "    output[8][19] = arr[0][244]\n",
        "    output[8][18] = arr[0][209]\n",
        "    output[8][17] = arr[0][171]\n",
        "    output[8][16] = arr[0][144]\n",
        "    output[8][15] = arr[0][112]\n",
        "    output[8][14] = arr[0][81]\n",
        "    output[8][13] = arr[0][54]\n",
        "    output[8][12] = arr[0][31]\n",
        "    output[8][11] = arr[0][14]\n",
        "    output[8][10] = arr[0][1]\n",
        "    output[8][9] = arr[0][8]\n",
        "    output[8][8] = arr[0][23]\n",
        "    output[8][7] = arr[0][42]\n",
        "    output[8][6] = arr[0][67]\n",
        "    output[8][5] = arr[0][96]\n",
        "    output[8][4] = arr[0][128]\n",
        "    output[8][3] = arr[0][156]\n",
        "    output[8][2] = arr[0][196]\n",
        "    output[8][1] = arr[0][231]\n",
        "\n",
        "    # 1\n",
        "    output[9][19] = arr[0][243]\n",
        "    output[9][18] = arr[0][208]\n",
        "    output[9][17] = arr[0][170]\n",
        "    output[9][16] = arr[0][143]\n",
        "    output[9][15] = arr[0][111]\n",
        "    output[9][14] = arr[0][80]\n",
        "    output[9][13] = arr[0][53]\n",
        "    output[9][12] = arr[0][30]\n",
        "    output[9][11] = arr[0][13]\n",
        "    output[9][10] = arr[0][0]\n",
        "    output[9][9] = arr[0][9]\n",
        "    output[9][8] = arr[0][24]\n",
        "    output[9][7] = arr[0][43]\n",
        "    output[9][6] = arr[0][68]\n",
        "    output[9][5] = arr[0][97]\n",
        "    output[9][4] = arr[0][129]\n",
        "    output[9][3] = arr[0][157]\n",
        "    output[9][2] = arr[0][197]\n",
        "    output[9][1] = arr[0][232]\n",
        "\n",
        "    # 12\n",
        "    output[10][18] = arr[0][225]\n",
        "    output[10][17] = arr[0][191]\n",
        "    output[10][16] = arr[0][142]\n",
        "    output[10][15] = arr[0][110]\n",
        "    output[10][14] = arr[0][79]\n",
        "    output[10][13] = arr[0][52]\n",
        "    output[10][12] = arr[0][29]\n",
        "    output[10][11] = arr[0][12]\n",
        "    output[10][10] = arr[0][11]\n",
        "    output[10][9] = arr[0][10]\n",
        "    output[10][8] = arr[0][25]\n",
        "    output[10][7] = arr[0][44]\n",
        "    output[10][6] = arr[0][69]\n",
        "    output[10][5] = arr[0][98]\n",
        "    output[10][4] = arr[0][130]\n",
        "    output[10][3] = arr[0][179]\n",
        "    output[10][2] = arr[0][213]\n",
        "\n",
        "    # 28\n",
        "    output[11][16] = arr[0][169]\n",
        "    output[11][15] = arr[0][141]\n",
        "    output[11][14] = arr[0][109]\n",
        "    output[11][13] = arr[0][78]\n",
        "    output[11][12] = arr[0][51]\n",
        "    output[11][11] = arr[0][28]\n",
        "    output[11][10] = arr[0][27]\n",
        "    output[11][9] = arr[0][26]\n",
        "    output[11][8] = arr[0][45]\n",
        "    output[11][7] = arr[0][70]\n",
        "    output[11][6] = arr[0][99]\n",
        "    output[11][5] = arr[0][131]\n",
        "    output[11][4] = arr[0][158]\n",
        "\n",
        "    # 49\n",
        "    output[12][17] = arr[0][190]\n",
        "    output[12][16] = arr[0][168]\n",
        "    output[12][15] = arr[0][140]\n",
        "    output[12][14] = arr[0][108]\n",
        "    output[12][13] = arr[0][77]\n",
        "    output[12][12] = arr[0][50]\n",
        "    output[12][11] = arr[0][49]\n",
        "    output[12][10] = arr[0][48]\n",
        "    output[12][9] = arr[0][47]\n",
        "    output[12][8] = arr[0][46]\n",
        "    output[12][7] = arr[0][71]\n",
        "    output[12][6] = arr[0][100]\n",
        "    output[12][5] = arr[0][132]\n",
        "    output[12][4] = arr[0][159]\n",
        "    output[12][3] = arr[0][180]\n",
        "\n",
        "    # 75\n",
        "    output[13][18] = arr[0][224]\n",
        "    output[13][17] = arr[0][207]\n",
        "    output[13][16] = arr[0][189]\n",
        "    output[13][15] = arr[0][167]\n",
        "    output[13][14] = arr[0][139]\n",
        "    output[13][13] = arr[0][107]\n",
        "    output[13][12] = arr[0][76]\n",
        "    output[13][11] = arr[0][75]\n",
        "    output[13][10] = arr[0][74]\n",
        "    output[13][9] = arr[0][73]\n",
        "    output[13][8] = arr[0][72]\n",
        "    output[13][7] = arr[0][101]\n",
        "    output[13][6] = arr[0][133]\n",
        "    output[13][5] = arr[0][160]\n",
        "    output[13][4] = arr[0][181]\n",
        "    output[13][3] = arr[0][198]\n",
        "    output[13][2] = arr[0][214]\n",
        "\n",
        "    # 105\n",
        "    output[14][18] = arr[0][242]\n",
        "    output[14][17] = arr[0][223]\n",
        "    output[14][16] = arr[0][206]\n",
        "    output[14][15] = arr[0][188]\n",
        "    output[14][14] = arr[0][166]\n",
        "    output[14][13] = arr[0][138]\n",
        "    output[14][12] = arr[0][106]\n",
        "    output[14][11] = arr[0][105]\n",
        "    output[14][10] = arr[0][104]\n",
        "    output[14][9] = arr[0][103]\n",
        "    output[14][8] = arr[0][102]\n",
        "    output[14][7] = arr[0][134]\n",
        "    output[14][6] = arr[0][161]\n",
        "    output[14][5] = arr[0][182]\n",
        "    output[14][4] = arr[0][199]\n",
        "    output[14][3] = arr[0][215]\n",
        "    output[14][2] = arr[0][233]\n",
        "\n",
        "    # 137\n",
        "    output[15][16] = arr[0][241]\n",
        "    output[15][15] = arr[0][222]\n",
        "    output[15][14] = arr[0][205]\n",
        "    output[15][13] = arr[0][187]\n",
        "    output[15][12] = arr[0][165]\n",
        "    output[15][11] = arr[0][137]\n",
        "    output[15][10] = arr[0][136]\n",
        "    output[15][9] = arr[0][135]\n",
        "    output[15][8] = arr[0][162]\n",
        "    output[15][7] = arr[0][183]\n",
        "    output[15][6] = arr[0][200]\n",
        "    output[15][5] = arr[0][216]\n",
        "    output[15][4] = arr[0][234]\n",
        "\n",
        "    # mix\n",
        "    output[16][15] = arr[0][240]\n",
        "    output[16][14] = arr[0][221]\n",
        "    output[16][13] = arr[0][204]\n",
        "    output[16][12] = arr[0][186]\n",
        "    output[16][11] = arr[0][164]\n",
        "    output[16][10] = arr[0][163]\n",
        "    output[16][9] = arr[0][184]\n",
        "    output[16][8] = arr[0][201]\n",
        "    output[16][7] = arr[0][217]\n",
        "    output[16][6] = arr[0][235]\n",
        "\n",
        "    # 186\n",
        "    output[17][12] = arr[0][220]\n",
        "    output[17][11] = arr[0][203]\n",
        "    output[17][10] = arr[0][185]\n",
        "    output[17][9] = arr[0][202]\n",
        "    output[17][8] = arr[0][218]\n",
        "\n",
        "    # 220\n",
        "    output[18][11] = arr[0][239]\n",
        "    output[18][10] = arr[0][219]\n",
        "    output[18][9] = arr[0][236]\n",
        "\n",
        "    # mix\n",
        "    output[19][11] = arr[0][238]\n",
        "    output[19][10] = arr[0][237]\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMoSYdGdDNLU"
      },
      "source": [
        "All training data is loaded into memory and fitted to the model along with the training lables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "Mj4vRsejDNLV"
      },
      "outputs": [],
      "source": [
        "def load_data_LTSM(dir_path):\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    for filename in os.listdir(dir_path):\n",
        "        if filename.endswith(\".h5\"):\n",
        "            filename_path = os.path.join(dir_path, filename)\n",
        "            with h5py.File(filename_path, \"r\") as f:\n",
        "                dataset_name = getdatasetname(filename_path)\n",
        "                label = get_label(filename)\n",
        "                matrix = f.get(dataset_name)[()]\n",
        "\n",
        "                train_meg = downsample(matrix, 113)\n",
        "                train_meg = scale(train_meg)\n",
        "                ############ Create mesh\n",
        "                meshes_list = []\n",
        "                arr = train_meg\n",
        "\n",
        "                # Iterate through the original array in chunks\n",
        "                for sensors in train_meg.T:\n",
        "                    sensors = np.reshape(sensors, (1, 248))\n",
        "                    mesh = array_to_mesh(sensors)\n",
        "\n",
        "                    # Append the resulting array to the list\n",
        "                    meshes_list.append(mesh)\n",
        "\n",
        "                train_meg = meshes_list\n",
        "                ##############\n",
        "\n",
        "                data.append(train_meg)\n",
        "                labels.append(label)\n",
        "    print(np.shape(data))\n",
        "\n",
        "    return np.array(data), np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxnfmv15DNLV",
        "outputId": "b8dd980c-b5c2-4c0e-8164-f64b0f1a708f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 1980, 20, 21)\n",
            "(8, 1980, 20, 21)\n",
            "(8, 1980, 20, 21)\n",
            "[[1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]]\n"
          ]
        }
      ],
      "source": [
        "trainx, trainY = load_data_LTSM(intra_train_path)\n",
        "\n",
        "trainy = tf.keras.utils.to_categorical(trainY)\n",
        "\n",
        "testx, testY = load_data_LTSM(intra_test_path)\n",
        "\n",
        "testy = tf.keras.utils.to_categorical(testY)\n",
        "\n",
        "print(testx.shape)\n",
        "print(testy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmNipaooD2o7",
        "outputId": "cfe38fa5-b1fa-4c07-8378-9f7061d53641"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8, 4)"
            ]
          },
          "execution_count": 145,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "testy.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJ7vaHpK5QOo",
        "outputId": "409030be-16ac-4b7d-e862-d1592b74d34b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Shape: (32, 1980, 20, 21)\n",
            "Reshaped Shape: (32, 20, 21, 1980)\n",
            "Reshaped Shape testx: (8, 20, 21, 1980)\n"
          ]
        }
      ],
      "source": [
        "reshaped_trainx = tf.transpose(trainx, perm=[0, 2, 3, 1])\n",
        "reshaped_testx = tf.transpose(testx, perm=[0, 2, 3, 1])\n",
        "# Print the shapes\n",
        "print(\"Original Shape:\", trainx.shape)\n",
        "print(\"Reshaped Shape:\", reshaped_trainx.shape)\n",
        "print(\"Reshaped Shape testx:\", reshaped_testx.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "QURQWLFS9K0b"
      },
      "outputs": [],
      "source": [
        "# \"\"\"\"model.add(tf.keras.layers.Reshape((model.output_shape[1], model.output_shape[2] * model.output_shape[3])))\n",
        "# #####\n",
        "\n",
        "# model.add(tf.keras.layers.LSTM(248, input_shape=(reshaped_trainx.shape[1],reshaped_trainx.shape[2])))\n",
        "# model.add(tf.keras.layers.Dropout(0.5))\n",
        "# model.add(tf.keras.layers.Dense(248, activation='relu'))\"\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_TOf-UYDNLV",
        "outputId": "d2c17a14-5e54-4a3b-f1a7-0eebe59454da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_24 (Conv2D)          (None, 20, 21, 128)       2281088   \n",
            "                                                                 \n",
            " conv2d_25 (Conv2D)          (None, 20, 21, 128)       147584    \n",
            "                                                                 \n",
            " max_pooling2d_12 (MaxPooli  (None, 10, 10, 128)       0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_26 (Conv2D)          (None, 10, 10, 256)       295168    \n",
            "                                                                 \n",
            " conv2d_27 (Conv2D)          (None, 10, 10, 256)       590080    \n",
            "                                                                 \n",
            " max_pooling2d_13 (MaxPooli  (None, 5, 5, 256)         0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 6400)              0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 128)               819328    \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 4)                 516       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4133764 (15.77 MB)\n",
            "Trainable params: 4133764 (15.77 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/129\n",
            "Epoch 2/129\n",
            "Epoch 3/129\n",
            "Epoch 4/129\n",
            "Epoch 5/129\n",
            "Epoch 6/129\n",
            "Epoch 7/129\n",
            "Epoch 8/129\n",
            "Epoch 9/129\n",
            "Epoch 10/129\n",
            "Epoch 11/129\n",
            "Epoch 12/129\n",
            "Epoch 13/129\n",
            "Epoch 14/129\n",
            "Epoch 15/129\n",
            "Epoch 16/129\n",
            "Epoch 17/129\n",
            "Epoch 18/129\n",
            "Epoch 19/129\n",
            "Epoch 20/129\n",
            "Epoch 21/129\n",
            "Epoch 22/129\n",
            "Epoch 23/129\n",
            "Epoch 24/129\n",
            "Epoch 25/129\n",
            "Epoch 26/129\n",
            "Epoch 27/129\n",
            "Epoch 28/129\n",
            "Epoch 29/129\n",
            "Epoch 30/129\n",
            "Epoch 31/129\n",
            "Epoch 32/129\n",
            "Epoch 33/129\n",
            "Epoch 34/129\n",
            "Epoch 35/129\n",
            "Epoch 36/129\n",
            "Epoch 37/129\n",
            "Epoch 38/129\n",
            "Epoch 39/129\n",
            "Epoch 40/129\n",
            "Epoch 41/129\n",
            "Epoch 42/129\n",
            "Epoch 43/129\n",
            "Epoch 44/129\n",
            "Epoch 45/129\n",
            "Epoch 46/129\n",
            "Epoch 47/129\n",
            "Epoch 48/129\n",
            "Epoch 49/129\n",
            "Epoch 50/129\n",
            "Epoch 51/129\n",
            "Epoch 52/129\n",
            "Epoch 53/129\n",
            "Epoch 54/129\n",
            "Epoch 55/129\n",
            "Epoch 56/129\n",
            "Epoch 57/129\n",
            "Epoch 58/129\n",
            "Epoch 59/129\n",
            "Epoch 60/129\n",
            "Epoch 61/129\n",
            "Epoch 62/129\n",
            "Epoch 63/129\n",
            "Epoch 64/129\n",
            "Epoch 65/129\n",
            "Epoch 66/129\n",
            "Epoch 67/129\n",
            "Epoch 68/129\n",
            "Epoch 69/129\n",
            "Epoch 70/129\n",
            "Epoch 71/129\n",
            "Epoch 72/129\n",
            "Epoch 73/129\n",
            "Epoch 74/129\n",
            "Epoch 75/129\n",
            "Epoch 76/129\n",
            "Epoch 77/129\n",
            "Epoch 78/129\n",
            "Epoch 79/129\n",
            "Epoch 80/129\n",
            "Epoch 81/129\n",
            "Epoch 82/129\n",
            "Epoch 83/129\n",
            "Epoch 84/129\n",
            "Epoch 85/129\n",
            "Epoch 86/129\n",
            "Epoch 87/129\n",
            "Epoch 88/129\n",
            "Epoch 89/129\n",
            "Epoch 90/129\n",
            "Epoch 91/129\n",
            "Epoch 92/129\n",
            "Epoch 93/129\n",
            "Epoch 94/129\n",
            "Epoch 95/129\n",
            "Epoch 96/129\n",
            "Epoch 97/129\n",
            "Epoch 98/129\n",
            "Epoch 99/129\n",
            "Epoch 100/129\n",
            "Epoch 101/129\n",
            "Epoch 102/129\n",
            "Epoch 103/129\n",
            "Epoch 104/129\n",
            "Epoch 105/129\n",
            "Epoch 106/129\n",
            "Epoch 107/129\n",
            "Epoch 108/129\n",
            "Epoch 109/129\n",
            "Epoch 110/129\n",
            "Epoch 111/129\n",
            "Epoch 112/129\n",
            "Epoch 113/129\n",
            "Epoch 114/129\n",
            "Epoch 115/129\n",
            "Epoch 116/129\n",
            "Epoch 117/129\n",
            "Epoch 118/129\n",
            "Epoch 119/129\n",
            "Epoch 120/129\n",
            "Epoch 121/129\n",
            "Epoch 122/129\n",
            "Epoch 123/129\n",
            "Epoch 124/129\n",
            "Epoch 125/129\n",
            "Epoch 126/129\n",
            "Epoch 127/129\n",
            "Epoch 128/129\n",
            "Epoch 129/129\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x23911b0c590>"
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "##### Add convolutional layers\n",
        "# Convolutional layers\n",
        "model.add(\n",
        "    tf.keras.layers.Conv2D(\n",
        "        128, (3, 3), activation=\"relu\", padding=\"same\", input_shape=(20, 21, 1980)\n",
        "    )\n",
        ")\n",
        "model.add(tf.keras.layers.Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
        "model.add(tf.keras.layers.Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "model.add(tf.keras.layers.Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(128, activation=\"relu\"))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(trainy.shape[1], activation=\"softmax\"))\n",
        "model.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.0001),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "verbose, epochs, batch_size = 16, 129, 64\n",
        "\n",
        "model.summary()\n",
        "model.fit(\n",
        "    reshaped_trainx, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train Accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = model.predict(reshaped_trainx)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "true_labels = np.argmax(trainy, axis=1)\n",
        "\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "print(cm)\n",
        "_, accuracy = model.evaluate(reshaped_trainx, trainy, batch_size=batch_size, verbose=0)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test Accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yGku4DL2J0m",
        "outputId": "26213548-86ed-49e6-dd38-f875d0a7fbf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 534ms/step\n",
            "[[2 0 0 0]\n",
            " [0 2 0 0]\n",
            " [0 0 2 0]\n",
            " [0 0 0 2]]\n",
            "1.0\n"
          ]
        }
      ],
      "source": [
        "predictions = model.predict(reshaped_testx)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "true_labels = np.argmax(testy, axis=1)\n",
        "\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "print(cm)\n",
        "_, accuracy = model.evaluate(reshaped_testx, testy, batch_size=batch_size, verbose=0)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cross model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "F0_P3m43HxyR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 1980, 20, 21)\n"
          ]
        }
      ],
      "source": [
        "trainx, trainY = load_data_LTSM(cross_train_path)\n",
        "\n",
        "trainy = tf.keras.utils.to_categorical(trainY)\n",
        "\n",
        "reshaped_trainx = tf.transpose(trainx, perm=[0, 2, 3, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/129\n",
            "Epoch 2/129\n",
            "Epoch 3/129\n",
            "Epoch 4/129\n",
            "Epoch 5/129\n",
            "Epoch 6/129\n",
            "Epoch 7/129\n",
            "Epoch 8/129\n",
            "Epoch 9/129\n",
            "Epoch 10/129\n",
            "Epoch 11/129\n",
            "Epoch 12/129\n",
            "Epoch 13/129\n",
            "Epoch 14/129\n",
            "Epoch 15/129\n",
            "Epoch 16/129\n",
            "Epoch 17/129\n",
            "Epoch 18/129\n",
            "Epoch 19/129\n",
            "Epoch 20/129\n",
            "Epoch 21/129\n",
            "Epoch 22/129\n",
            "Epoch 23/129\n",
            "Epoch 24/129\n",
            "Epoch 25/129\n",
            "Epoch 26/129\n",
            "Epoch 27/129\n",
            "Epoch 28/129\n",
            "Epoch 29/129\n",
            "Epoch 30/129\n",
            "Epoch 31/129\n",
            "Epoch 32/129\n",
            "Epoch 33/129\n",
            "Epoch 34/129\n",
            "Epoch 35/129\n",
            "Epoch 36/129\n",
            "Epoch 37/129\n",
            "Epoch 38/129\n",
            "Epoch 39/129\n",
            "Epoch 40/129\n",
            "Epoch 41/129\n",
            "Epoch 42/129\n",
            "Epoch 43/129\n",
            "Epoch 44/129\n",
            "Epoch 45/129\n",
            "Epoch 46/129\n",
            "Epoch 47/129\n",
            "Epoch 48/129\n",
            "Epoch 49/129\n",
            "Epoch 50/129\n",
            "Epoch 51/129\n",
            "Epoch 52/129\n",
            "Epoch 53/129\n",
            "Epoch 54/129\n",
            "Epoch 55/129\n",
            "Epoch 56/129\n",
            "Epoch 57/129\n",
            "Epoch 58/129\n",
            "Epoch 59/129\n",
            "Epoch 60/129\n",
            "Epoch 61/129\n",
            "Epoch 62/129\n",
            "Epoch 63/129\n",
            "Epoch 64/129\n",
            "Epoch 65/129\n",
            "Epoch 66/129\n",
            "Epoch 67/129\n",
            "Epoch 68/129\n",
            "Epoch 69/129\n",
            "Epoch 70/129\n",
            "Epoch 71/129\n",
            "Epoch 72/129\n",
            "Epoch 73/129\n",
            "Epoch 74/129\n",
            "Epoch 75/129\n",
            "Epoch 76/129\n",
            "Epoch 77/129\n",
            "Epoch 78/129\n",
            "Epoch 79/129\n",
            "Epoch 80/129\n",
            "Epoch 81/129\n",
            "Epoch 82/129\n",
            "Epoch 83/129\n",
            "Epoch 84/129\n",
            "Epoch 85/129\n",
            "Epoch 86/129\n",
            "Epoch 87/129\n",
            "Epoch 88/129\n",
            "Epoch 89/129\n",
            "Epoch 90/129\n",
            "Epoch 91/129\n",
            "Epoch 92/129\n",
            "Epoch 93/129\n",
            "Epoch 94/129\n",
            "Epoch 95/129\n",
            "Epoch 96/129\n",
            "Epoch 97/129\n",
            "Epoch 98/129\n",
            "Epoch 99/129\n",
            "Epoch 100/129\n",
            "Epoch 101/129\n",
            "Epoch 102/129\n",
            "Epoch 103/129\n",
            "Epoch 104/129\n",
            "Epoch 105/129\n",
            "Epoch 106/129\n",
            "Epoch 107/129\n",
            "Epoch 108/129\n",
            "Epoch 109/129\n",
            "Epoch 110/129\n",
            "Epoch 111/129\n",
            "Epoch 112/129\n",
            "Epoch 113/129\n",
            "Epoch 114/129\n",
            "Epoch 115/129\n",
            "Epoch 116/129\n",
            "Epoch 117/129\n",
            "Epoch 118/129\n",
            "Epoch 119/129\n",
            "Epoch 120/129\n",
            "Epoch 121/129\n",
            "Epoch 122/129\n",
            "Epoch 123/129\n",
            "Epoch 124/129\n",
            "Epoch 125/129\n",
            "Epoch 126/129\n",
            "Epoch 127/129\n",
            "Epoch 128/129\n",
            "Epoch 129/129\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x2391008a850>"
            ]
          },
          "execution_count": 151,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(\n",
        "    reshaped_trainx, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train Accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = model.predict(reshaped_trainx)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "true_labels = np.argmax(trainy, axis=1)\n",
        "\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "print(cm)\n",
        "_, accuracy = model.evaluate(reshaped_trainx, trainy, batch_size=batch_size, verbose=0)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cross Test 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(16, 1980, 20, 21)\n"
          ]
        }
      ],
      "source": [
        "testx, testY = load_data_LTSM(cross_test1_path)\n",
        "\n",
        "testy = tf.keras.utils.to_categorical(testY)\n",
        "\n",
        "reshaped_testx = tf.transpose(testx, perm=[0, 2, 3, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "[[4 0 0 0]\n",
            " [0 4 0 0]\n",
            " [0 0 4 0]\n",
            " [0 0 0 4]]\n",
            "1.0\n"
          ]
        }
      ],
      "source": [
        "predictions = model.predict(reshaped_testx)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "true_labels = np.argmax(testy, axis=1)\n",
        "\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "print(cm)\n",
        "_, accuracy = model.evaluate(reshaped_testx, testy, batch_size=batch_size, verbose=0)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cross test 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(16, 1980, 20, 21)\n"
          ]
        }
      ],
      "source": [
        "testx, testY = load_data_LTSM(cross_test2_path)\n",
        "\n",
        "testy = tf.keras.utils.to_categorical(testY)\n",
        "\n",
        "reshaped_testx = tf.transpose(testx, perm=[0, 2, 3, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 877ms/step\n",
            "[[4 0 0 0]\n",
            " [0 0 3 1]\n",
            " [0 0 0 4]\n",
            " [0 0 1 3]]\n",
            "0.4375\n"
          ]
        }
      ],
      "source": [
        "predictions = model.predict(reshaped_testx)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "true_labels = np.argmax(testy, axis=1)\n",
        "\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "print(cm)\n",
        "_, accuracy = model.evaluate(reshaped_testx, testy, batch_size=batch_size, verbose=0)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cross test 3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(16, 1980, 20, 21)\n"
          ]
        }
      ],
      "source": [
        "testx, testY = load_data_LTSM(cross_test3_path)\n",
        "\n",
        "testy = tf.keras.utils.to_categorical(testY)\n",
        "\n",
        "reshaped_testx = tf.transpose(testx, perm=[0, 2, 3, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 856ms/step\n",
            "[[4 0 0 0]\n",
            " [0 1 2 1]\n",
            " [0 0 4 0]\n",
            " [0 0 1 3]]\n",
            "0.75\n"
          ]
        }
      ],
      "source": [
        "predictions = model.predict(reshaped_testx)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "true_labels = np.argmax(testy, axis=1)\n",
        "\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "print(cm)\n",
        "_, accuracy = model.evaluate(reshaped_testx, testy, batch_size=batch_size, verbose=0)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternative approach: Add LSTM layers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_28 (Conv2D)          (None, 20, 21, 128)       2281088   \n",
            "                                                                 \n",
            " conv2d_29 (Conv2D)          (None, 20, 21, 128)       147584    \n",
            "                                                                 \n",
            " max_pooling2d_14 (MaxPooli  (None, 10, 10, 128)       0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_30 (Conv2D)          (None, 10, 10, 256)       295168    \n",
            "                                                                 \n",
            " conv2d_31 (Conv2D)          (None, 10, 10, 256)       590080    \n",
            "                                                                 \n",
            " max_pooling2d_15 (MaxPooli  (None, 5, 5, 256)         0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " reshape_1 (Reshape)         (None, 5, 1280)           0         \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 128)               721408    \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 4)                 516       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4052356 (15.46 MB)\n",
            "Trainable params: 4052356 (15.46 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/129\n",
            "Epoch 2/129\n",
            "Epoch 3/129\n",
            "Epoch 4/129\n",
            "Epoch 5/129\n",
            "Epoch 6/129\n",
            "Epoch 7/129\n",
            "Epoch 8/129\n",
            "Epoch 9/129\n",
            "Epoch 10/129\n",
            "Epoch 11/129\n",
            "Epoch 12/129\n",
            "Epoch 13/129\n",
            "Epoch 14/129\n",
            "Epoch 15/129\n",
            "Epoch 16/129\n",
            "Epoch 17/129\n",
            "Epoch 18/129\n",
            "Epoch 19/129\n",
            "Epoch 20/129\n",
            "Epoch 21/129\n",
            "Epoch 22/129\n",
            "Epoch 23/129\n",
            "Epoch 24/129\n",
            "Epoch 25/129\n",
            "Epoch 26/129\n",
            "Epoch 27/129\n",
            "Epoch 28/129\n",
            "Epoch 29/129\n",
            "Epoch 30/129\n",
            "Epoch 31/129\n",
            "Epoch 32/129\n",
            "Epoch 33/129\n",
            "Epoch 34/129\n",
            "Epoch 35/129\n",
            "Epoch 36/129\n",
            "Epoch 37/129\n",
            "Epoch 38/129\n",
            "Epoch 39/129\n",
            "Epoch 40/129\n",
            "Epoch 41/129\n",
            "Epoch 42/129\n",
            "Epoch 43/129\n",
            "Epoch 44/129\n",
            "Epoch 45/129\n",
            "Epoch 46/129\n",
            "Epoch 47/129\n",
            "Epoch 48/129\n",
            "Epoch 49/129\n",
            "Epoch 50/129\n",
            "Epoch 51/129\n",
            "Epoch 52/129\n",
            "Epoch 53/129\n",
            "Epoch 54/129\n",
            "Epoch 55/129\n",
            "Epoch 56/129\n",
            "Epoch 57/129\n",
            "Epoch 58/129\n",
            "Epoch 59/129\n",
            "Epoch 60/129\n",
            "Epoch 61/129\n",
            "Epoch 62/129\n",
            "Epoch 63/129\n",
            "Epoch 64/129\n",
            "Epoch 65/129\n",
            "Epoch 66/129\n",
            "Epoch 67/129\n",
            "Epoch 68/129\n",
            "Epoch 69/129\n",
            "Epoch 70/129\n",
            "Epoch 71/129\n",
            "Epoch 72/129\n",
            "Epoch 73/129\n",
            "Epoch 74/129\n",
            "Epoch 75/129\n",
            "Epoch 76/129\n",
            "Epoch 77/129\n",
            "Epoch 78/129\n",
            "Epoch 79/129\n",
            "Epoch 80/129\n",
            "Epoch 81/129\n",
            "Epoch 82/129\n",
            "Epoch 83/129\n",
            "Epoch 84/129\n",
            "Epoch 85/129\n",
            "Epoch 86/129\n",
            "Epoch 87/129\n",
            "Epoch 88/129\n",
            "Epoch 89/129\n",
            "Epoch 90/129\n",
            "Epoch 91/129\n",
            "Epoch 92/129\n",
            "Epoch 93/129\n",
            "Epoch 94/129\n",
            "Epoch 95/129\n",
            "Epoch 96/129\n",
            "Epoch 97/129\n",
            "Epoch 98/129\n",
            "Epoch 99/129\n",
            "Epoch 100/129\n",
            "Epoch 101/129\n",
            "Epoch 102/129\n",
            "Epoch 103/129\n",
            "Epoch 104/129\n",
            "Epoch 105/129\n",
            "Epoch 106/129\n",
            "Epoch 107/129\n",
            "Epoch 108/129\n",
            "Epoch 109/129\n",
            "Epoch 110/129\n",
            "Epoch 111/129\n",
            "Epoch 112/129\n",
            "Epoch 113/129\n",
            "Epoch 114/129\n",
            "Epoch 115/129\n",
            "Epoch 116/129\n",
            "Epoch 117/129\n",
            "Epoch 118/129\n",
            "Epoch 119/129\n",
            "Epoch 120/129\n",
            "Epoch 121/129\n",
            "Epoch 122/129\n",
            "Epoch 123/129\n",
            "Epoch 124/129\n",
            "Epoch 125/129\n",
            "Epoch 126/129\n",
            "Epoch 127/129\n",
            "Epoch 128/129\n",
            "Epoch 129/129\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x23912dc4650>"
            ]
          },
          "execution_count": 158,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "##### Add convolutional layers\n",
        "# Convolutional layers\n",
        "model.add(\n",
        "    tf.keras.layers.Conv2D(\n",
        "        128, (3, 3), activation=\"relu\", padding=\"same\", input_shape=(20, 21, 1980)\n",
        "    )\n",
        ")\n",
        "model.add(tf.keras.layers.Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
        "model.add(tf.keras.layers.Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "model.add(tf.keras.layers.Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
        "model.add(tf.keras.layers.Reshape((5, 5 * 256)))\n",
        "model.add(tf.keras.layers.LSTM(128))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(128, activation=\"relu\"))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(trainy.shape[1], activation=\"softmax\"))\n",
        "model.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.0001),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "verbose, epochs, batch_size = 16, 129, 64\n",
        "\n",
        "model.summary()\n",
        "model.fit(\n",
        "    reshaped_trainx, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test alternative model on intra\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 1980, 20, 21)\n",
            "(8, 1980, 20, 21)\n"
          ]
        }
      ],
      "source": [
        "trainx, trainY = load_data_LTSM(intra_train_path)\n",
        "\n",
        "trainy = tf.keras.utils.to_categorical(trainY)\n",
        "\n",
        "testx, testY = load_data_LTSM(intra_test_path)\n",
        "\n",
        "testy = tf.keras.utils.to_categorical(testY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {},
      "outputs": [],
      "source": [
        "reshaped_trainx = tf.transpose(trainx, perm=[0, 2, 3, 1])\n",
        "reshaped_testx = tf.transpose(testx, perm=[0, 2, 3, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "[[2 0 0 0]\n",
            " [0 0 0 2]\n",
            " [0 0 0 2]\n",
            " [0 0 0 2]]\n",
            "0.5\n"
          ]
        }
      ],
      "source": [
        "predictions = model.predict(reshaped_testx)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "true_labels = np.argmax(testy, axis=1)\n",
        "\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "print(cm)\n",
        "_, accuracy = model.evaluate(reshaped_testx, testy, batch_size=batch_size, verbose=0)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test Alternative model on cross\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 1980, 20, 21)\n"
          ]
        }
      ],
      "source": [
        "trainx, trainY = load_data_LTSM(cross_train_path)\n",
        "\n",
        "trainy = tf.keras.utils.to_categorical(trainY)\n",
        "\n",
        "reshaped_trainx = tf.transpose(trainx, perm=[0, 2, 3, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/129\n",
            "Epoch 2/129\n",
            "Epoch 3/129\n",
            "Epoch 4/129\n",
            "Epoch 5/129\n",
            "Epoch 6/129\n",
            "Epoch 7/129\n",
            "Epoch 8/129\n",
            "Epoch 9/129\n",
            "Epoch 10/129\n",
            "Epoch 11/129\n",
            "Epoch 12/129\n",
            "Epoch 13/129\n",
            "Epoch 14/129\n",
            "Epoch 15/129\n",
            "Epoch 16/129\n",
            "Epoch 17/129\n",
            "Epoch 18/129\n",
            "Epoch 19/129\n",
            "Epoch 20/129\n",
            "Epoch 21/129\n",
            "Epoch 22/129\n",
            "Epoch 23/129\n",
            "Epoch 24/129\n",
            "Epoch 25/129\n",
            "Epoch 26/129\n",
            "Epoch 27/129\n",
            "Epoch 28/129\n",
            "Epoch 29/129\n",
            "Epoch 30/129\n",
            "Epoch 31/129\n",
            "Epoch 32/129\n",
            "Epoch 33/129\n",
            "Epoch 34/129\n",
            "Epoch 35/129\n",
            "Epoch 36/129\n",
            "Epoch 37/129\n",
            "Epoch 38/129\n",
            "Epoch 39/129\n",
            "Epoch 40/129\n",
            "Epoch 41/129\n",
            "Epoch 42/129\n",
            "Epoch 43/129\n",
            "Epoch 44/129\n",
            "Epoch 45/129\n",
            "Epoch 46/129\n",
            "Epoch 47/129\n",
            "Epoch 48/129\n",
            "Epoch 49/129\n",
            "Epoch 50/129\n",
            "Epoch 51/129\n",
            "Epoch 52/129\n",
            "Epoch 53/129\n",
            "Epoch 54/129\n",
            "Epoch 55/129\n",
            "Epoch 56/129\n",
            "Epoch 57/129\n",
            "Epoch 58/129\n",
            "Epoch 59/129\n",
            "Epoch 60/129\n",
            "Epoch 61/129\n",
            "Epoch 62/129\n",
            "Epoch 63/129\n",
            "Epoch 64/129\n",
            "Epoch 65/129\n",
            "Epoch 66/129\n",
            "Epoch 67/129\n",
            "Epoch 68/129\n",
            "Epoch 69/129\n",
            "Epoch 70/129\n",
            "Epoch 71/129\n",
            "Epoch 72/129\n",
            "Epoch 73/129\n",
            "Epoch 74/129\n",
            "Epoch 75/129\n",
            "Epoch 76/129\n",
            "Epoch 77/129\n",
            "Epoch 78/129\n",
            "Epoch 79/129\n",
            "Epoch 80/129\n",
            "Epoch 81/129\n",
            "Epoch 82/129\n",
            "Epoch 83/129\n",
            "Epoch 84/129\n",
            "Epoch 85/129\n",
            "Epoch 86/129\n",
            "Epoch 87/129\n",
            "Epoch 88/129\n",
            "Epoch 89/129\n",
            "Epoch 90/129\n",
            "Epoch 91/129\n",
            "Epoch 92/129\n",
            "Epoch 93/129\n",
            "Epoch 94/129\n",
            "Epoch 95/129\n",
            "Epoch 96/129\n",
            "Epoch 97/129\n",
            "Epoch 98/129\n",
            "Epoch 99/129\n",
            "Epoch 100/129\n",
            "Epoch 101/129\n",
            "Epoch 102/129\n",
            "Epoch 103/129\n",
            "Epoch 104/129\n",
            "Epoch 105/129\n",
            "Epoch 106/129\n",
            "Epoch 107/129\n",
            "Epoch 108/129\n",
            "Epoch 109/129\n",
            "Epoch 110/129\n",
            "Epoch 111/129\n",
            "Epoch 112/129\n",
            "Epoch 113/129\n",
            "Epoch 114/129\n",
            "Epoch 115/129\n",
            "Epoch 116/129\n",
            "Epoch 117/129\n",
            "Epoch 118/129\n",
            "Epoch 119/129\n",
            "Epoch 120/129\n",
            "Epoch 121/129\n",
            "Epoch 122/129\n",
            "Epoch 123/129\n",
            "Epoch 124/129\n",
            "Epoch 125/129\n",
            "Epoch 126/129\n",
            "Epoch 127/129\n",
            "Epoch 128/129\n",
            "Epoch 129/129\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x239115594d0>"
            ]
          },
          "execution_count": 163,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(\n",
        "    reshaped_trainx, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternative Cross Test 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(16, 1980, 20, 21)\n"
          ]
        }
      ],
      "source": [
        "testx, testY = load_data_LTSM(cross_test1_path)\n",
        "\n",
        "testy = tf.keras.utils.to_categorical(testY)\n",
        "\n",
        "reshaped_testx = tf.transpose(testx, perm=[0, 2, 3, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "[[4 0 0 0]\n",
            " [0 0 4 0]\n",
            " [0 0 4 0]\n",
            " [0 0 4 0]]\n",
            "0.5\n"
          ]
        }
      ],
      "source": [
        "predictions = model.predict(reshaped_testx)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "true_labels = np.argmax(testy, axis=1)\n",
        "\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "print(cm)\n",
        "_, accuracy = model.evaluate(reshaped_testx, testy, batch_size=batch_size, verbose=0)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternative Cross Test 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(16, 1980, 20, 21)\n"
          ]
        }
      ],
      "source": [
        "testx, testY = load_data_LTSM(cross_test2_path)\n",
        "\n",
        "testy = tf.keras.utils.to_categorical(testY)\n",
        "\n",
        "reshaped_testx = tf.transpose(testx, perm=[0, 2, 3, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "[[4 0 0 0]\n",
            " [0 0 4 0]\n",
            " [0 1 3 0]\n",
            " [0 0 4 0]]\n",
            "0.4375\n"
          ]
        }
      ],
      "source": [
        "predictions = model.predict(reshaped_testx)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "true_labels = np.argmax(testy, axis=1)\n",
        "\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "print(cm)\n",
        "_, accuracy = model.evaluate(reshaped_testx, testy, batch_size=batch_size, verbose=0)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternative Cross test 3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(16, 1980, 20, 21)\n"
          ]
        }
      ],
      "source": [
        "testx, testY = load_data_LTSM(cross_test3_path)\n",
        "\n",
        "testy = tf.keras.utils.to_categorical(testY)\n",
        "\n",
        "reshaped_testx = tf.transpose(testx, perm=[0, 2, 3, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "[[4 0 0 0]\n",
            " [0 0 4 0]\n",
            " [0 0 4 0]\n",
            " [0 0 4 0]]\n",
            "0.5\n"
          ]
        }
      ],
      "source": [
        "predictions = model.predict(reshaped_testx)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "true_labels = np.argmax(testy, axis=1)\n",
        "\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "print(cm)\n",
        "_, accuracy = model.evaluate(reshaped_testx, testy, batch_size=batch_size, verbose=0)\n",
        "print(accuracy)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "a79e8111ab597848eb35ce52217a0592e18d0e165d5b30c34dd3cf158fcb803c"
    },
    "kernelspec": {
      "display_name": "Python 3.9.18 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
