{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(248, 35624)\n",
      "[[0.75709679 0.75655099 0.75461831 ... 0.75787271 0.75722762 0.76213064]\n",
      " [0.57519353 0.56595376 0.55152292 ... 0.48226206 0.48329766 0.48382933]\n",
      " [0.76639813 0.76220723 0.75976352 ... 0.73835103 0.73913462 0.74171893]\n",
      " ...\n",
      " [0.78054736 0.78005406 0.77447601 ... 0.78332417 0.77990748 0.78139679]\n",
      " [0.5291242  0.52498386 0.51793955 ... 0.47161298 0.46912766 0.47366007]\n",
      " [0.49634087 0.48903129 0.47857    ... 0.5523971  0.55285888 0.55528368]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import h5py\n",
    "import numpy\n",
    "\n",
    "\n",
    "def get_dataset_name(file_name_with_dir):\n",
    "    file_name_without_dir = file_name_with_dir.split(\"/\")[-1]\n",
    "    temp = file_name_without_dir.split(\"_\")[:-1]\n",
    "    dataset_name = \"_\".join(temp)\n",
    "    return dataset_name\n",
    "\n",
    "\n",
    "# replace with local path\n",
    "filename_path = \"data/pdata/Intra/train/rest_105923_1.h5\"\n",
    "with h5py.File(filename_path, \"r\") as f:\n",
    "    dataset_name = get_dataset_name(filename_path)\n",
    "    matrix = f.get(dataset_name)[()]\n",
    "    print(type(matrix))\n",
    "    print(matrix.shape)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(matrix)\n",
    "scaled_data = scaler.transform(matrix)\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARRAY TO MESH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import boto3\n",
    "import shutil\n",
    "import numpy as np\n",
    "from os.path import isdir,isfile,join,exists\n",
    "from os import mkdir,makedirs,getcwd,listdir\n",
    "import mne\n",
    "import reading_raw\n",
    "import gc\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from multiprocessing import Pool\n",
    "from scipy import stats\n",
    "\n",
    "def normalize(array):\n",
    "    return stats.zscore(array)\n",
    "\n",
    "#Given the number \"n\", it finds the closest that is divisible by \"m\"\n",
    "#Used when splitting the matrices\n",
    "def closestNumber(n, m) : \n",
    "    q = int(n / m) \n",
    "    n1 = m * q \n",
    "    if((n * m) > 0) : \n",
    "        n2 = (m * (q + 1))  \n",
    "    else : \n",
    "        n2 = (m * (q - 1)) \n",
    "    if (abs(n - n1) < abs(n - n2)) : \n",
    "        return n1 \n",
    "    return n2 \n",
    "\n",
    "     \n",
    "#Reads the binary file given the subject and the type of state\n",
    "#If the reading is successful, returns the matrix of size (248,number_time_steps)\n",
    "#If the reading is NOT successful, prints the problem and returns the boolean False\n",
    "def get_raw_data(subject, type_state, hcp_path):\n",
    "  try: #type of state for this subject might not exist\n",
    "    print(\"Reading the binary file and returning the raw matrix ...\")\n",
    "    raw = reading_raw.read_raw(subject=subject, hcp_path=hcp_path, run_index=0, data_type=type_state)\n",
    "    raw.load_data()\n",
    "    meg_picks = mne.pick_types(raw.info, meg=True, ref_meg=False)\n",
    "    raw_matrix = raw[meg_picks[:]][0]\n",
    "    del raw\n",
    "    return raw_matrix\n",
    "  except Exception as e:\n",
    "    print(\"Problem in reading the file: The type of state '{}' might not be there for subject '{}'\".format(type_state, subject))\n",
    "    print(\"Exception error : \",e)\n",
    "    return False\n",
    "\n",
    "\n",
    "def create_data_directory():\n",
    "    print(\"Creating data directory or skipping if already existing...\")\n",
    "    if(not isdir(\"Data\")):\n",
    "        try:\n",
    "            mkdir(\"Data\") \n",
    "            print(\"Created Data folder !\")\n",
    "        except Exception as e:\n",
    "            print (\"Creation of the Data directory failed\")\n",
    "            print(\"Exception error: \",str(e))\n",
    "            \n",
    "    if(not isdir(\"Data/train\")):\n",
    "        try:\n",
    "            mkdir(\"Data/train\")    \n",
    "            print(\"Created train folder !\")\n",
    "        except Exception as e:\n",
    "            print (\"Creation of the train directory failed\")\n",
    "            print(\"Exception error: \",str(e))\n",
    "            \n",
    "    if(not isdir(\"Data/validate\")):\n",
    "        try:\n",
    "            mkdir(\"Data/validate\")  \n",
    "            print(\"Created validate folder !\")\n",
    "        except Exception as e:\n",
    "            print (\"Creation of the validate directory failed\")\n",
    "            print(\"Exception error: \",str(e))\n",
    "            \n",
    "    if(not isdir(\"Data/test\")):\n",
    "        try:\n",
    "            mkdir(\"Data/test\")    \n",
    "            print(\"Created test folder !\")\n",
    "        except Exception as e:\n",
    "            print (\"Creation of the test directory failed\")\n",
    "            print(\"Exception error: \",str(e))\n",
    "\n",
    "\n",
    "def create_h5_files(raw_matrix,subject,type_state):\n",
    "    print()\n",
    "    print(\"shape of raw matrix\",raw_matrix.shape)\n",
    "    print()\n",
    "    train_folder = \"Data/train/\"\n",
    "    validate_folder = \"Data/validate/\"\n",
    "    test_folder = \"Data/test/\"\n",
    "    \n",
    "    number_epochs = 250\n",
    "    time_steps_per_epoch = 1425\n",
    "    number_columns = number_epochs * time_steps_per_epoch\n",
    "\n",
    "    number_columns_per_chunk = number_columns // 10\n",
    "\n",
    "    if subject == \"212318\" or subject == \"162935\" or subject == \"204521\" or subject == \"601127\" or subject == \"725751\" or subject == \"735148\":  \n",
    "        #data goes to test folder\n",
    "        for i in range(10):\n",
    "            start_index_col = number_columns_per_chunk * (i+4) # i+4 corresponds to an offset of 30s (approximately) from  the start\n",
    "            stop_index_col = start_index_col + number_columns_per_chunk - 1 \n",
    "            destination_file = test_folder + type_state+'_'+subject+'_'+str(i+1)+'.h5'\n",
    "            with h5py.File(destination_file, \"w\") as hf:\n",
    "                    hf.create_dataset(type_state+'_'+subject, data=raw_matrix[ : , start_index_col : stop_index_col ])       \n",
    "        \n",
    "    else:\n",
    "        #data goes to train and validate folder\n",
    "        for i in range(10):\n",
    "            if i >= 0 and i < 8:\n",
    "                destination_file = train_folder + type_state+'_'+subject+'_'+str(i+1)+'.h5'\n",
    "            if i >= 8 and i < 10:\n",
    "                destination_file = validate_folder + type_state+'_'+subject+'_'+str(i+1)+'.h5'\n",
    "            start_index_col = number_columns_per_chunk * (i+4) # i+4 corresponds to an offset of 30s (approximately) from  the start\n",
    "            stop_index_col = start_index_col + number_columns_per_chunk - 1\n",
    "            with h5py.File(destination_file, \"w\") as hf:\n",
    "                hf.create_dataset(type_state+'_'+subject, data=raw_matrix[ : , start_index_col : stop_index_col ])      \n",
    "\n",
    "##For each subject, it prints how many rest files and how many task files it has in the Amazon server            \n",
    "def get_info_files_subjects(personal_access_key_id,secret_access_key, subjects):\n",
    "    folders = [\"3-Restin\",\"4-Restin\",\"5-Restin\",\"6-Wrkmem\",\"7-Wrkmem\",\"8-StoryM\",\"9-StoryM\",\"10-Motort\",\"11-Motort\"]\n",
    "    s3 = boto3.client('s3', aws_access_key_id=personal_access_key_id, aws_secret_access_key=secret_access_key)\n",
    "    for subject in subjects:\n",
    "        rest_count = 0\n",
    "        task_count = 0\n",
    "        for folder in folders:\n",
    "            number_files = s3.list_objects_v2(Bucket=\"hcp-openaccess\", Prefix='HCP_1200/'+subject+'/unprocessed/MEG/'+folder)['KeyCount']\n",
    "            if \"Restin\" in folder:\n",
    "                rest_count += number_files\n",
    "            else:\n",
    "                task_count += number_files\n",
    "        print(\"for subject {}, rest_count = {}, and task_count = {}\".format(subject, rest_count, task_count))\n",
    "        \n",
    "#Gets a list of subjects and returns a new list of subjects that might contain less subjects\n",
    "#It iterates through all the subjects, and if a subject has 0 task or rest state files, it discards the subject\n",
    "#Used to fix data imbalance and to prevent bugs during the training using the data generator\n",
    "def get_filtered_subjects(personal_access_key_id,secret_access_key, subjects):\n",
    "    print(\"starting to discard subjects to keep data balance for multi class classification...\")\n",
    "    print(\"\\nA subject that contains every data should have 12 files for rest and 36 files for task\\n\")\n",
    "    new_subject_list = []\n",
    "    number_subjects = len(subjects)\n",
    "    folders = [\"3-Restin\",\"4-Restin\",\"5-Restin\",\"6-Wrkmem\",\"7-Wrkmem\",\"8-StoryM\",\"9-StoryM\",\"10-Motort\",\"11-Motort\"]\n",
    "    s3 = boto3.client('s3', aws_access_key_id=personal_access_key_id, aws_secret_access_key=secret_access_key)\n",
    "    for subject in subjects:\n",
    "        rest_count = 0\n",
    "        task_count = 0\n",
    "        for folder in folders:\n",
    "            number_files = s3.list_objects_v2(Bucket=\"hcp-openaccess\", Prefix='HCP_1200/'+subject+'/unprocessed/MEG/'+folder)['KeyCount']\n",
    "            if \"Restin\" in folder:\n",
    "                rest_count += number_files\n",
    "            else:\n",
    "                task_count += number_files\n",
    "        if (rest_count == 12 and task_count == 36):\n",
    "            new_subject_list.append(subject)\n",
    "        else:\n",
    "            print(\"Discarding subject '{}' because it had {} rest files and {} task files\".format(subject, rest_count, task_count))\n",
    "\n",
    "\n",
    "    new_list_len = len(new_subject_list)\n",
    "    print(\"-\"*7 + \" Done filtering out subjects ! \" + \"-\"*7)\n",
    "    print(\"Original list had {} subjects and new list has {} subjects\".format(number_subjects, new_list_len))\n",
    "    return new_subject_list\n",
    "\n",
    "###Downloads 1 subject and ignores the case where 1 of the folders/files is missing    \n",
    "def download_subject(subject,personal_access_key_id,secret_access_key):\n",
    "  s3 = boto3.client('s3', aws_access_key_id=personal_access_key_id, aws_secret_access_key=secret_access_key)\n",
    "\n",
    "  folders = [\"3-Restin\",\"4-Restin\",\"5-Restin\",\"6-Wrkmem\",\"7-Wrkmem\",\"8-StoryM\",\"9-StoryM\",\"10-Motort\",\"11-Motort\"]\n",
    "  filenames = [\"c,rfDC\", \"config\", \"e,rfhp1.0Hz,COH\", \"e,rfhp1.0Hz,COH1\"]\n",
    "\n",
    "  print(\"Creating the directories for the subject '{}'\".format(subject))\n",
    "  print()\n",
    "  if exists(getcwd()+\"//\"+subject) == False:\n",
    "    for folder in folders:\n",
    "        makedirs(subject+\"/unprocessed/MEG/\"+folder+\"/4D/\")\n",
    "  print(\"done !\")\n",
    "  print()\n",
    "  print(\"Will start downloading the following files for all folders:\")\n",
    "  print(filenames)\n",
    "  print()\n",
    "  print()\n",
    "  for filename in filenames:\n",
    "    for folder in folders:\n",
    "      if filename == \"c,rfDC\":\n",
    "        print(\"downloading c,rfDC file for folder {} ...\".format(folder))\n",
    "        print()\n",
    "      if(exists(getcwd()+\"//\"+subject+'/unprocessed/MEG/'+folder+'/4D/'+filename)):\n",
    "        print(\"File already exists, moving on ...\")\n",
    "        print()\n",
    "        pass\n",
    "      try:\n",
    "        s3.download_file('hcp-openaccess', 'HCP_1200/'+subject+'/unprocessed/MEG/'+folder+'/4D/'+filename, subject+'/unprocessed/MEG/'+folder+'/4D/'+filename)\n",
    "        if filename == \"c,rfDC\":\n",
    "          print(\"done downloading c,rfDC for folder {} !\".format(folder))\n",
    "          print()\n",
    "      except Exception as e:\n",
    "        print()\n",
    "        print(\"the folder '{}' for subject '{}' does not exist in Amazon server, moving to next folder ...\".format(folder,subject))\n",
    "        print(\"Exception error message: \"+str(e))\n",
    "        pass\n",
    "    \n",
    "\n",
    "#Main function to be executed to download subjects\n",
    "#list_subjects should be a list of strings containing the 6 digits subjects\n",
    "#hcp_path should be the current working directory (os.getcwd())\n",
    "def download_batch_subjects(list_subjects, personal_access_key_id, secret_access_key, hcp_path): # hcp_path should be os.getcwd()\n",
    "  create_data_directory()  \n",
    "  state_types = [\"rest\", \"task_working_memory\", \"task_story_math\", \"task_motor\"]\n",
    "  for subject in list_subjects:\n",
    "    download_subject(subject,personal_access_key_id,secret_access_key)\n",
    "    for state in state_types:\n",
    "      matrix_raw = get_raw_data(subject, state, hcp_path)\n",
    "      if type(matrix_raw) != type(False): # if the reading was done successfully\n",
    "        print()\n",
    "        print(\"Creating the uncompressed h5 files ...\")\n",
    "        create_h5_files(matrix_raw,subject,state)\n",
    "    print(\"done creating the uncompressed h5 files for subject '{}' !\".format(subject))\n",
    "    print()\n",
    "    print(\"deleting the directory containing the binary files of subject '{}' ...\".format(subject))\n",
    "    print()\n",
    "    try:\n",
    "      shutil.rmtree(subject+\"/\",ignore_errors=True)#Removes the folder and all folders/files inside\n",
    "      print(\"Done deleting the directory of the binary files!\")\n",
    "      print(\"Moving on to the next subject ...\")\n",
    "      print()\n",
    "    except Exception as e :\n",
    "      print()\n",
    "      print(\"Error while trying to delete the directory.\")\n",
    "      print(\"Exception message : \" + str(e))\n",
    "      \n",
    "\n",
    "def separate_list(all_files_list):\n",
    "    rest_list = []\n",
    "    mem_list = []\n",
    "    math_list = []\n",
    "    motor_list = []\n",
    "    for item in all_files_list:\n",
    "        if \"rest\" in item:\n",
    "            rest_list.append(item)\n",
    "        if \"memory\" in item:\n",
    "            mem_list.append(item)\n",
    "        if \"math\" in item:\n",
    "            math_list.append(item)\n",
    "        if \"motor\" in item:\n",
    "            motor_list.append(item)            \n",
    "    return rest_list, mem_list, math_list, motor_list\n",
    "\n",
    "def order_arranging(rest_list,mem_list,math_list,motor_list):\n",
    "    ordered_list = []\n",
    "    for index, (value1, value2, value3, value4) in enumerate(zip(rest_list, mem_list, math_list, motor_list)):\n",
    "        ordered_list.append(value1)\n",
    "        ordered_list.append(value2)\n",
    "        ordered_list.append(value3)\n",
    "        ordered_list.append(value4)\n",
    "    return ordered_list\n",
    "\n",
    "def multi_processing_cascade(directory, length, num_cpu,depth):\n",
    "    \n",
    "    assert len(directory) == length*num_cpu,\"Directory does not have {} files.\".format(length*num_cpu)\n",
    "    window_size = 10\n",
    "    input_rows = 20\n",
    "    input_columns = 21\n",
    "    split = []\n",
    "\n",
    "    for i in range(num_cpu):\n",
    "        split.append(directory[i*length:(i*length)+length])\n",
    "        \n",
    "    for i in range(len(split)):\n",
    "        split[i] = (split[i],depth)\n",
    "\n",
    "    pool = Pool(num_cpu)\n",
    "    results = pool.map(load_overlapped_data_cascade, split)\n",
    "    pool.terminate()\n",
    "    pool.join()\n",
    "    \n",
    "    y = np.random.rand(1,4)\n",
    "    for i in range(len(results)):\n",
    "        y = np.concatenate((y,results[i][1]))\n",
    "\n",
    "    y = np.delete(y,0,0)\n",
    "    gc.collect()\n",
    "    \n",
    "    x={}\n",
    "\n",
    "    x_temp = np.random.rand(1,input_rows,input_columns,depth)\n",
    "    for i in range(window_size):\n",
    "        for j in range(len(results)):\n",
    "            x_temp = np.concatenate((x_temp,results[j][0][\"input\"+str(i+1)]))\n",
    "        x_temp= np.delete(x_temp,0,0)\n",
    "        x[\"input\"+str(i+1)] = x_temp\n",
    "        x_temp = np.random.rand(1,input_rows,input_columns,depth)\n",
    "        gc.collect()\n",
    "    return x, y\n",
    "\n",
    "def multi_processing_multiview(directory, length, num_cpu,depth):\n",
    "    assert len(directory) == length*num_cpu,\"Directory does not have {} files.\".format(length*num_cpu)\n",
    "    window_size = 10\n",
    "    input_rows = 20\n",
    "    input_columns = 21\n",
    "    input_channels = 248\n",
    "    split = []\n",
    "\n",
    "    for i in range(num_cpu):\n",
    "        split.append(directory[i*length:(i*length)+length])\n",
    "        \n",
    "    for i in range(len(split)):\n",
    "        split[i] = (split[i],depth)\n",
    "\n",
    "    pool = Pool(num_cpu)\n",
    "    results = pool.map(load_overlapped_data_multiview, split)\n",
    "    pool.terminate()\n",
    "    pool.join()\n",
    "\n",
    "    y = np.random.rand(1,4)\n",
    "    for i in range(len(results)):\n",
    "        y = np.concatenate((y,results[i][1]))\n",
    "\n",
    "    y = np.delete(y,0,0)\n",
    "    gc.collect()\n",
    "    \n",
    "    x={}\n",
    "\n",
    "    x_temp = np.random.rand(1,input_rows,input_columns,depth)\n",
    "    x_lstm = np.random.rand(1,input_channels,depth)\n",
    "    for i in range(window_size):\n",
    "        for j in range(len(results)):\n",
    "            x_temp = np.concatenate((x_temp,results[j][0][\"input\"+str(i+1)]))\n",
    "            x_lstm = np.concatenate((x_lstm,results[j][0][\"input\"+str(i+window_size+1)]))\n",
    "\n",
    "        x_temp= np.delete(x_temp,0,0)\n",
    "        x_lstm= np.delete(x_lstm,0,0)\n",
    "        x[\"input\"+str(i+1)] = x_temp\n",
    "        x[\"input\"+str(i+window_size+1)] = x_lstm\n",
    "        x_temp = np.random.rand(1,input_rows,input_columns,depth)\n",
    "        x_lstm = np.random.rand(1,input_channels,depth)\n",
    "        gc.collect()\n",
    "\n",
    "    return x, y\n",
    "      \n",
    "\n",
    "def get_lists_indexes(matrix_length,window_size):\n",
    "    indexes=[]\n",
    "    for i in range(window_size):\n",
    "        indexes.append(np.arange(start=i, stop = matrix_length-(window_size-1-i),step = 5,dtype=np.int64))\n",
    "    return indexes\n",
    "\n",
    "\n",
    "def get_dataset_name(file_name_with_dir):\n",
    "    filename_without_dir = file_name_with_dir.split('/')[-1]\n",
    "    temp = filename_without_dir.split('_')[:-1]\n",
    "    dataset_name = \"_\".join(temp)\n",
    "    return dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eigen code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_type(matrix, window_size,depth):\n",
    "    input_rows = 20\n",
    "    input_columns = 21\n",
    "    input_channels = 248\n",
    "\n",
    "    if(matrix.shape[1] == 1):\n",
    "        length = 1\n",
    "    else:\n",
    "        length = closestNumber(int(matrix.shape[1]) - window_size*depth,window_size*depth)\n",
    "\n",
    "    meshes = np.zeros((input_rows,input_columns,length),dtype=np.float64)\n",
    "    for i in range(length):\n",
    "        array_time_step = np.reshape(matrix[:,i],(1,input_channels))\n",
    "        meshes[:,:,i] = array_to_mesh(array_time_step)\n",
    "    \n",
    "    del matrix    \n",
    "\n",
    "    inputs = []\n",
    "    if length == 1:\n",
    "        for i in range(window_size):\n",
    "            inputs.append(np.zeros((0,input_rows,input_columns,depth)))\n",
    "    else:\n",
    "        column_offset = int(window_size*depth/2)    # difference between values in columns\n",
    "        num_rows_big_matrix = int((length-window_size*depth/2)/column_offset) # number of rows\n",
    "\n",
    "        for j in range(num_rows_big_matrix):\n",
    "            if j == 0:\n",
    "                for i in range(window_size):\n",
    "                    inputs.append(np.zeros((num_rows_big_matrix,input_rows,input_columns,depth)))\n",
    "                    inputs[i][j] = meshes[:,:,i*depth:(i+1)*depth]\n",
    "            else:\n",
    "                for i in range(window_size):\n",
    "                    inputs[i][j] = meshes[:,:,column_offset*j+i*depth:column_offset*j+(i+1)*depth]\n",
    "\n",
    "    del meshes\n",
    "    gc.collect()\n",
    "    \n",
    "    number_y_labels = int((length/(window_size*depth)*2)-1)\n",
    "    y = np.ones((number_y_labels,1),dtype=np.int8)\n",
    "    return inputs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    \n",
    "def preprocess_data_type(matrix, window_size,depth):\n",
    "    input_rows = 20\n",
    "    input_columns = 21\n",
    "    input_channels = 248\n",
    "\n",
    "    if(matrix.shape[1] == 1):\n",
    "        length = 1\n",
    "    else:\n",
    "        length = closestNumber(int(matrix.shape[1]) - window_size*depth,window_size*depth)\n",
    "\n",
    "    meshes = np.zeros((input_rows,input_columns,length),dtype=np.float64)\n",
    "    for i in range(length):\n",
    "        array_time_step = np.reshape(matrix[:,i],(1,input_channels))\n",
    "        meshes[:,:,i] = array_to_mesh(array_time_step)\n",
    "    \n",
    "    del matrix    \n",
    "\n",
    "    inputs = []\n",
    "    if length == 1:\n",
    "        for i in range(window_size):\n",
    "            inputs.append(np.zeros((0,input_rows,input_columns,depth)))\n",
    "    else:\n",
    "        column_offset = int(window_size*depth/2)    # difference between values in columns\n",
    "        num_rows_big_matrix = int((length-window_size*depth/2)/column_offset) # number of rows\n",
    "\n",
    "        for j in range(num_rows_big_matrix):\n",
    "            if j == 0:\n",
    "                for i in range(window_size):\n",
    "                    inputs.append(np.zeros((num_rows_big_matrix,input_rows,input_columns,depth)))\n",
    "                    inputs[i][j] = meshes[:,:,i*depth:(i+1)*depth]\n",
    "            else:\n",
    "                for i in range(window_size):\n",
    "                    inputs[i][j] = meshes[:,:,column_offset*j+i*depth:column_offset*j+(i+1)*depth]\n",
    "\n",
    "    del meshes\n",
    "    gc.collect()\n",
    "    \n",
    "    number_y_labels = int((length/(window_size*depth)*2)-1)\n",
    "    y = np.ones((number_y_labels,1),dtype=np.int8)\n",
    "    return inputs, y\n",
    "\n",
    "def preprocess_data_type_lstm(matrix,window_size,depth):\n",
    "    input_channels = 248\n",
    "\n",
    "    if(matrix.shape[1] == 1):\n",
    "        length = 1\n",
    "    else:\n",
    "        length = closestNumber(int(matrix.shape[1]) - window_size*depth,window_size*depth)\n",
    "        \n",
    "    matrices = np.zeros((input_channels,length),dtype=np.float64)\n",
    "    for i in range(length):\n",
    "        matrix_step=np.reshape(matrix[:,i],(1,input_channels))\n",
    "        matrices[:,i] = matrix_step\n",
    "    \n",
    "    del matrix\n",
    "\n",
    "    inputs = []\n",
    "    if length == 1:\n",
    "        for i in range(window_size):\n",
    "            inputs.append(np.zeros((0,input_channels,depth)))\n",
    "    else:      \n",
    "        var = int(window_size*depth/2)    # difference between values in columns\n",
    "        var2 = int((length-window_size*depth/2)/var) # number of rows\n",
    "\n",
    "        for j in range(var2):\n",
    "            if j == 0:\n",
    "                for i in range(window_size):\n",
    "                    inputs.append(np.zeros((var2,input_channels,depth)))\n",
    "                    inputs[i][j] = matrices[:,i*depth:(i+1)*depth]\n",
    "            else:\n",
    "                for i in range(window_size):\n",
    "                    inputs[i][j] = matrices[:,var*j+i*depth:var*j+(i+1)*depth]\n",
    "\n",
    "    del matrices\n",
    "    gc.collect()\n",
    "\n",
    "    number_y_labels = int((length/(window_size*depth)*2)-1)\n",
    "    y = np.ones((number_y_labels,1),dtype=np.int8)\n",
    "    return inputs, y\n",
    "\n",
    "def reshape_input_dictionary(input_dict, output_list, batch_size,depth):\n",
    "\n",
    "    input_rows = 20\n",
    "    input_columns = 21\n",
    "    input_channels = 248\n",
    "    length_training = output_list.shape[0]\n",
    "    length_adapted_batch_size= closestNumber(length_training-batch_size,batch_size)\n",
    "\n",
    "    for i in range(len(input_dict.keys())):\n",
    "        if i < 10:\n",
    "            input_dict[\"input\"+str(i+1)] = np.reshape(input_dict[\"input\"+str(i+1)][0:length_adapted_batch_size],(length_adapted_batch_size,input_rows,input_columns,depth))\n",
    "        else:\n",
    "            input_dict[\"input\"+str(i+1)] = np.reshape(input_dict[\"input\"+str(i+1)][0:length_adapted_batch_size],(length_adapted_batch_size,input_channels,depth))\n",
    "\n",
    "    output_list = output_list[0:length_adapted_batch_size]\n",
    "    return input_dict, output_list\n",
    "\n",
    "def load_overlapped_data_cascade(file_dirs_depth):\n",
    "    \n",
    "    input_rows = 20\n",
    "    input_columns = 21\n",
    "    input_channels = 248 #MEG channels\n",
    "    number_classes = 4\n",
    "    window_size = 10\n",
    "    depth = file_dirs_depth[1]\n",
    "\n",
    "    rest_matrix = np.random.rand(input_channels,1)\n",
    "    math_matrix = np.random.rand(input_channels,1)\n",
    "    memory_matrix = np.random.rand(input_channels,1)\n",
    "    motor_matrix = np.random.rand(input_channels,1)\n",
    " \n",
    "\n",
    "    files_to_load = file_dirs_depth[0]\n",
    "    \n",
    "    for i in range(len(files_to_load)):\n",
    "        if \"rest\" in files_to_load[i]:\n",
    "            with h5py.File(files_to_load[i],'r') as f:\n",
    "                dataset_name = get_dataset_name(files_to_load[i])\n",
    "                matrix = f.get(dataset_name)\n",
    "                matrix = np.array(matrix)\n",
    "            assert matrix.shape[0] == input_channels , \"This rest data does not have {} channels, but {} instead\".format(input_channels,matrix.shape[0])\n",
    "            rest_matrix = np.column_stack((rest_matrix, matrix))\n",
    "\n",
    "        if \"math\" in files_to_load[i]:\n",
    "            with h5py.File(files_to_load[i],'r') as f:\n",
    "                dataset_name = get_dataset_name(files_to_load[i])\n",
    "                matrix = f.get(dataset_name)\n",
    "                matrix = np.array(matrix)\n",
    "            assert matrix.shape[0] == input_channels , \"This math data does not have {} channels, but {} instead\".format(input_channels,matrix.shape[0])\n",
    "            math_matrix = np.column_stack((math_matrix, matrix))\n",
    "            \n",
    "        if \"memory\" in files_to_load[i]:\n",
    "            with h5py.File(files_to_load[i],'r') as f:\n",
    "                dataset_name = get_dataset_name(files_to_load[i])\n",
    "                matrix = f.get(dataset_name)\n",
    "                matrix = np.array(matrix)\n",
    "            assert matrix.shape[0] == input_channels , \"This memory data does not have {} channels, but {} instead\".format(input_channels,matrix.shape[0])\n",
    "            memory_matrix = np.column_stack((memory_matrix, matrix))\n",
    "            \n",
    "        if \"motor\" in files_to_load[i]:\n",
    "            with h5py.File(files_to_load[i],'r') as f:\n",
    "                dataset_name = get_dataset_name(files_to_load[i])\n",
    "                matrix = f.get(dataset_name)\n",
    "                matrix = np.array(matrix)\n",
    "            assert matrix.shape[0] == input_channels , \"This motor data does not have {} channels, but {} instead\".format(input_channels,matrix.shape[0])\n",
    "            motor_matrix = np.column_stack((motor_matrix, matrix))\n",
    "        matrix = None\n",
    "\n",
    "    x_rest,y_rest = preprocess_data_type(rest_matrix, window_size,depth) \n",
    "    rest_matrix = None\n",
    "    y_rest = y_rest*0\n",
    "\n",
    "    x_math,y_math = preprocess_data_type(math_matrix,window_size,depth)\n",
    "    math_matrix = None\n",
    "    gc.collect()\n",
    "\n",
    "    x_mem,y_mem = preprocess_data_type(memory_matrix,window_size,depth)\n",
    "    memory_matrix = None\n",
    "    y_mem = y_mem * 2\n",
    "   \n",
    "    x_motor,y_motor = preprocess_data_type(motor_matrix,window_size,depth)\n",
    "    motor_matrix = None\n",
    "    y_motor = y_motor * 3\n",
    "    gc.collect()\n",
    "       \n",
    "    dict_list = []\n",
    "    for i in range(window_size):\n",
    "        dict_list.append({0:x_rest[i], 1:x_math[i], 2:x_mem[i], 3:x_motor[i]})\n",
    "        x_rest[i]= None\n",
    "        x_math[i]= None\n",
    "        x_mem[i]= None\n",
    "        x_motor[i]= None\n",
    "        gc.collect()\n",
    " \n",
    "    inputs = []\n",
    "    for i in range(window_size):\n",
    "        inputs.append(np.random.rand(1,input_rows,input_columns,depth))\n",
    "\n",
    "    for i in range(number_classes):\n",
    "        for j in range(window_size):\n",
    "            if dict_list[j][i].shape[0]>0:\n",
    "                inputs[j]=np.concatenate((inputs[j],dict_list[j][i]))\n",
    "                \n",
    "    dict_list = None\n",
    "    gc.collect()\n",
    "    \n",
    "    for i in range(window_size):\n",
    "        inputs[i] = np.delete(inputs[i],0,0)\n",
    "    \n",
    "    dict_y = {0:y_rest,1:y_math,2:y_mem,3:y_motor}\n",
    "    \n",
    "    y = np.random.rand(1,1)\n",
    "    for i in range(number_classes):\n",
    "        if dict_y[i].shape[0]>0:\n",
    "            y = np.concatenate((y,dict_y[i]))\n",
    "\n",
    "\n",
    "    y = np.delete(y,0,0)\n",
    "\n",
    "    # inputs[0],inputs[1],inputs[2],inputs[3],inputs[4],inputs[5],inputs[6],inputs[7],inputs[8],inputs[9],y = shuffle(inputs[0],inputs[1],inputs[2],inputs[3],inputs[4],inputs[5],inputs[6],inputs[7],inputs[8],inputs[9],y,random_state=42)\n",
    "    *inputs,y = shuffle(*inputs,y,random_state=42)\n",
    "    x_length = inputs[0].shape[0]\n",
    "    for i in range(x_length):\n",
    "        for j in range(window_size):\n",
    "            temp = inputs[j][i] # length,rows,columns,depth\n",
    "            for k in range(depth):\n",
    "                inside = temp[:,:,k]\n",
    "                norm = normalize(inside)\n",
    "                inputs[j][i][:,:,k] = norm\n",
    "\n",
    "    temp = None\n",
    "    inside = None\n",
    "    norm = None\n",
    "    gc.collect()\n",
    "\n",
    "    data_dict = {'input1' : inputs[0], 'input2' : inputs[1],'input3' : inputs[2], 'input4': inputs[3], 'input5' : inputs[4],\n",
    "                 'input6' : inputs[5], 'input7' : inputs[6],'input8' : inputs[7], 'input9': inputs[8], 'input10' : inputs[9]}\n",
    "    \n",
    "    inputs = None\n",
    "    gc.collect()\n",
    "    y = to_categorical(y,number_classes)\n",
    "\n",
    "    return data_dict,y\n",
    "\n",
    "def load_overlapped_data_multiview(file_dirs_depth):\n",
    "\n",
    "    input_rows = 20\n",
    "    input_columns = 21\n",
    "    input_channels = 248 #MEG channels\n",
    "    number_classes = 4\n",
    "    window_size = 10\n",
    "    depth = file_dirs_depth[1]\n",
    "\n",
    "    rest_matrix = np.random.rand(input_channels,1)\n",
    "    math_matrix = np.random.rand(input_channels,1)\n",
    "    memory_matrix = np.random.rand(input_channels,1)\n",
    "    motor_matrix = np.random.rand(input_channels,1)\n",
    "\n",
    "    files_to_load = file_dirs_depth[0]\n",
    "\n",
    "    for i in range(len(files_to_load)):\n",
    "        if \"rest\" in files_to_load[i]:\n",
    "            with h5py.File(files_to_load[i],'r') as f:\n",
    "                dataset_name = get_dataset_name(files_to_load[i])\n",
    "                matrix = f.get(dataset_name)\n",
    "                matrix = np.array(matrix)\n",
    "            assert matrix.shape[0] == input_channels , \"This rest data does not have {} channels, but {} instead\".format(input_channels,matrix.shape[0])\n",
    "            rest_matrix = np.column_stack((rest_matrix, matrix))\n",
    "\n",
    "        if \"math\" in files_to_load[i]:\n",
    "            with h5py.File(files_to_load[i],'r') as f:\n",
    "                dataset_name = get_dataset_name(files_to_load[i])\n",
    "                matrix = f.get(dataset_name)\n",
    "                matrix = np.array(matrix)\n",
    "            assert matrix.shape[0] == input_channels , \"This math data does not have {} channels, but {} instead\".format(input_channels,matrix.shape[0])\n",
    "            math_matrix = np.column_stack((math_matrix, matrix))\n",
    "            \n",
    "        if \"memory\" in files_to_load[i]:\n",
    "            with h5py.File(files_to_load[i],'r') as f:\n",
    "                dataset_name = get_dataset_name(files_to_load[i])\n",
    "                matrix = f.get(dataset_name)\n",
    "                matrix = np.array(matrix)\n",
    "            assert matrix.shape[0] == input_channels , \"This memory data does not have {} channels, but {} instead\".format(input_channels,matrix.shape[0])\n",
    "            memory_matrix = np.column_stack((memory_matrix, matrix))\n",
    "            \n",
    "        if \"motor\" in files_to_load[i]:\n",
    "            with h5py.File(files_to_load[i],'r') as f:\n",
    "                dataset_name = get_dataset_name(files_to_load[i])\n",
    "                matrix = f.get(dataset_name)\n",
    "                matrix = np.array(matrix)\n",
    "            assert matrix.shape[0] == input_channels , \"This motor data does not have {} channels, but {} instead\".format(input_channels,matrix.shape[0])\n",
    "            motor_matrix = np.column_stack((motor_matrix, matrix))\n",
    "\n",
    "        matrix = None\n",
    "\n",
    "    x_rest,y_rest = preprocess_data_type(rest_matrix,window_size,depth)\n",
    "    x_rest_lstm,y_rest_lstm = preprocess_data_type_lstm(rest_matrix,window_size,depth)  \n",
    "    rest_matrix = None\n",
    "    y_rest = y_rest*0\n",
    "    y_rest_lstm = y_rest_lstm*0\n",
    "\n",
    "    x_math,y_math = preprocess_data_type(math_matrix,window_size,depth)\n",
    "    x_math_lstm,y_math_lstm = preprocess_data_type_lstm(math_matrix,window_size,depth)       \n",
    "    math_matrix = None\n",
    "    \n",
    "    x_mem,y_mem = preprocess_data_type(memory_matrix,window_size,depth)\n",
    "    x_mem_lstm,y_mem_lstm = preprocess_data_type_lstm(memory_matrix,window_size,depth)\n",
    "    memory_matrix = None\n",
    "    y_mem = y_mem*2\n",
    "    y_mem_lstm = y_mem_lstm*2\n",
    "    \n",
    "    x_motor,y_motor = preprocess_data_type(motor_matrix,window_size,depth)\n",
    "    x_motor_lstm,y_motor_lstm = preprocess_data_type_lstm(motor_matrix,window_size,depth)\n",
    "    motor_matrix = None\n",
    "    y_motor = y_motor * 3\n",
    "    y_motor_lstm = y_motor_lstm * 3\n",
    "    \n",
    "    dicts_cnn = []\n",
    "    dicts_lstm = []\n",
    "    for i in range(window_size):\n",
    "        dicts_cnn.append({0:x_rest[i], 1:x_math[i], 2:x_mem[i], 3:x_motor[i]})\n",
    "        dicts_lstm.append({0:x_rest_lstm[i], 1:x_math_lstm[i], 2:x_mem_lstm[i], 3:x_motor_lstm[i]})\n",
    "        x_rest[i]= None\n",
    "        x_math[i]= None\n",
    "        x_mem[i]= None\n",
    "        x_motor[i]= None\n",
    "        x_rest_lstm[i]= None\n",
    "        x_math_lstm[i]= None\n",
    "        x_mem_lstm[i]= None\n",
    "        x_motor_lstm[i]= None\n",
    "        gc.collect()\n",
    "\n",
    "    inputs = []\n",
    "    for i in range(window_size):\n",
    "        inputs.append(np.random.rand(1,input_rows,input_columns,depth))\n",
    "\n",
    "    for i in range(window_size):\n",
    "        inputs.append(np.random.rand(1,input_channels,depth))\n",
    "\n",
    "    for i in range(number_classes):\n",
    "        for j in range(window_size):\n",
    "            if dicts_cnn[j][i].shape[0]>0:\n",
    "                inputs[j]=np.concatenate((inputs[j],dicts_cnn[j][i]))\n",
    "                dicts_cnn[j][i] = None\n",
    "            if dicts_lstm[j][i].shape[0]>0:\n",
    "                inputs[j+window_size]=np.concatenate((inputs[j+window_size],dicts_lstm[j][i]))\n",
    "                dicts_lstm[j][i] = None\n",
    "            gc.collect()\n",
    "        \n",
    "    for i in range(window_size):\n",
    "        inputs[i] = np.delete(inputs[i],0,0)\n",
    "        inputs[i+window_size] = np.delete(inputs[i+window_size],0,0)\n",
    "        \n",
    "    dict_y = {0:y_rest,1:y_math,2:y_mem,3:y_motor}\n",
    "    \n",
    "    \n",
    "    y = np.random.rand(1,1)\n",
    "    for i in range(number_classes):\n",
    "        if dict_y[i].shape[0]>0:\n",
    "            y = np.concatenate((y,dict_y[i]))\n",
    "\n",
    "    y = np.delete(y,0,0)\n",
    "\n",
    "    # inputs[0],inputs[1],inputs[2],inputs[3],inputs[4],inputs[5],inputs[6],inputs[7],inputs[8],inputs[9],inputs[10],inputs[11],inputs[12],inputs[13],inputs[14],inputs[15],inputs[16],inputs[17],inputs[18],inputs[19],y = shuffle(inputs[0],inputs[1],inputs[2],inputs[3],inputs[4],inputs[5],inputs[6],inputs[7],inputs[8],inputs[9], inputs[10],inputs[11],inputs[12],inputs[13],inputs[14],inputs[15],inputs[16],inputs[17],inputs[18],inputs[19], y,random_state=42)\n",
    "    *inputs,y = shuffle(*inputs,y,random_state=42)\n",
    "    x_length = inputs[0].shape[0]\n",
    "    for i in range(x_length):\n",
    "        for j in range(window_size):\n",
    "            for k in range(depth):\n",
    "                temp = inputs[j][i]\n",
    "                inside = temp[:,:,k]\n",
    "                norm = normalize(inside)\n",
    "                inputs[j][i][:,:,k] = norm\n",
    "\n",
    "                temp = inputs[j+window_size][i]\n",
    "                inside = temp[:,k]\n",
    "                norm = normalize(inside)\n",
    "                inputs[j+window_size][i][:,k] = norm\n",
    "\n",
    "    del temp\n",
    "    del inside\n",
    "    del norm\n",
    "    gc.collect()\n",
    "    \n",
    "    data_dict = {'input1' : inputs[0], 'input2' : inputs[1],'input3' : inputs[2], 'input4': inputs[3], 'input5' : inputs[4],\n",
    "                 'input6' : inputs[5], 'input7' : inputs[6],'input8' : inputs[7], 'input9': inputs[8], 'input10' : inputs[9],\n",
    "                 'input11' : inputs[10], 'input12' : inputs[11],'input13' : inputs[12], 'input14': inputs[13], 'input15' : inputs[14],\n",
    "                 'input16' : inputs[15], 'input17' : inputs[16],'input18' : inputs[17], 'input19': inputs[18], 'input20' : inputs[19]}\n",
    "    \n",
    "    del inputs\n",
    "    gc.collect()\n",
    "    \n",
    "    y = to_categorical(y,number_classes)\n",
    "    return data_dict,y\n",
    "    \n",
    "    \n",
    "def array_to_mesh(arr):    \n",
    "\n",
    "    input_rows = 20\n",
    "    input_columns = 21\n",
    "    input_channels = 248\n",
    "\n",
    "    assert arr.shape == (1,input_channels),\"the shape of the input array should be (1,248) because there are 248 MEG channels,received array of shape \" + str(arr.shape)\n",
    "    output = np.zeros((input_rows,input_columns),dtype = np.float)\n",
    "    \n",
    "    #121\n",
    "    output[0][10] = arr[0][120]\n",
    "      \n",
    "    #89\n",
    "    output[1][12] = arr[0][151]\n",
    "    output[1][11] = arr[0][119]\n",
    "    output[1][10] = arr[0][88]\n",
    "    output[1][9] = arr[0][89]\n",
    "    output[1][8] = arr[0][121]\n",
    "    \n",
    "    #61\n",
    "    output[2][13] = arr[0][150]\n",
    "    output[2][12] = arr[0][118]\n",
    "    output[2][11] = arr[0][87]\n",
    "    output[2][10] = arr[0][60]\n",
    "    output[2][9] = arr[0][61]\n",
    "    output[2][8] = arr[0][90]\n",
    "    output[2][7] = arr[0][122]\n",
    "    \n",
    "    #37\n",
    "    output[3][14] = arr[0][149]\n",
    "    output[3][13] = arr[0][117]\n",
    "    output[3][12] = arr[0][86]\n",
    "    output[3][11] = arr[0][59]\n",
    "    output[3][10] = arr[0][36]\n",
    "    output[3][9] = arr[0][37]\n",
    "    output[3][8] = arr[0][62]\n",
    "    output[3][7] = arr[0][91]\n",
    "    output[3][6] = arr[0][123]\n",
    "    \n",
    "    #19\n",
    "    output[4][17] = arr[0][194]\n",
    "    output[4][16] = arr[0][175]\n",
    "    output[4][15] = arr[0][148]\n",
    "    output[4][14] = arr[0][116]\n",
    "    output[4][13] = arr[0][85]\n",
    "    output[4][12] = arr[0][58]\n",
    "    output[4][11] = arr[0][35]\n",
    "    output[4][10] = arr[0][18]\n",
    "    output[4][9] = arr[0][19]\n",
    "    output[4][8] = arr[0][38]\n",
    "    output[4][7] = arr[0][63]\n",
    "    output[4][6] = arr[0][92]\n",
    "    output[4][5] = arr[0][152]\n",
    "    output[4][4] = arr[0][176]\n",
    "\n",
    "    #5\n",
    "    output[5][20] = arr[0][247]\n",
    "    output[5][19] = arr[0][227]\n",
    "    output[5][18] = arr[0][193]\n",
    "    output[5][17] = arr[0][174]\n",
    "    output[5][16] = arr[0][147]\n",
    "    output[5][15] = arr[0][115]\n",
    "    output[5][14] = arr[0][84]\n",
    "    output[5][13] = arr[0][57]\n",
    "    output[5][12] = arr[0][34]\n",
    "    output[5][11] = arr[0][17]\n",
    "    output[5][10] = arr[0][4]\n",
    "    output[5][9] = arr[0][5]\n",
    "    output[5][8] = arr[0][20]\n",
    "    output[5][7] = arr[0][39]\n",
    "    output[5][6] = arr[0][64]\n",
    "    output[5][5] = arr[0][93]\n",
    "    output[5][4] = arr[0][125]\n",
    "    output[5][3] = arr[0][153]\n",
    "    output[5][2] = arr[0][177]\n",
    "    output[5][1] = arr[0][211]\n",
    "    output[5][0] = arr[0][228]\n",
    "    \n",
    "    #4\n",
    "    output[6][20] = arr[0][246]\n",
    "    output[6][19] = arr[0][226]\n",
    "    output[6][18] = arr[0][192]\n",
    "    output[6][17] = arr[0][173]\n",
    "    output[6][16] = arr[0][146]\n",
    "    output[6][15] = arr[0][114]\n",
    "    output[6][14] = arr[0][83]\n",
    "    output[6][13] = arr[0][56]\n",
    "    output[6][12] = arr[0][33]\n",
    "    output[6][11] = arr[0][16]\n",
    "    output[6][10] = arr[0][3]\n",
    "    output[6][9] = arr[0][6]\n",
    "    output[6][8] = arr[0][21]\n",
    "    output[6][7] = arr[0][40]\n",
    "    output[6][6] = arr[0][65]\n",
    "    output[6][5] = arr[0][94]\n",
    "    output[6][4] = arr[0][126]\n",
    "    output[6][3] = arr[0][154]\n",
    "    output[6][2] = arr[0][178]\n",
    "    output[6][1] = arr[0][212]\n",
    "    output[6][0] = arr[0][229]\n",
    "\n",
    "    \n",
    "    #3\n",
    "    output[7][19] = arr[0][245]\n",
    "    output[7][18] = arr[0][210]\n",
    "    output[7][17] = arr[0][172]\n",
    "    output[7][16] = arr[0][145]\n",
    "    output[7][15] = arr[0][113]\n",
    "    output[7][14] = arr[0][82]\n",
    "    output[7][13] = arr[0][55]\n",
    "    output[7][12] = arr[0][32]\n",
    "    output[7][11] = arr[0][15]\n",
    "    output[7][10] = arr[0][2]\n",
    "    output[7][9] = arr[0][7]\n",
    "    output[7][8] = arr[0][22]\n",
    "    output[7][7] = arr[0][41]\n",
    "    output[7][6] = arr[0][66]\n",
    "    output[7][5] = arr[0][95]\n",
    "    output[7][4] = arr[0][127]\n",
    "    output[7][3] = arr[0][155]\n",
    "    output[7][2] = arr[0][195]\n",
    "    output[7][1] = arr[0][230]\n",
    "            \n",
    "    #8\n",
    "    output[8][19] = arr[0][244]\n",
    "    output[8][18] = arr[0][209]\n",
    "    output[8][17] = arr[0][171]\n",
    "    output[8][16] = arr[0][144]\n",
    "    output[8][15] = arr[0][112]\n",
    "    output[8][14] = arr[0][81]\n",
    "    output[8][13] = arr[0][54]\n",
    "    output[8][12] = arr[0][31]\n",
    "    output[8][11] = arr[0][14]\n",
    "    output[8][10] = arr[0][1]\n",
    "    output[8][9] = arr[0][8]\n",
    "    output[8][8] = arr[0][23]\n",
    "    output[8][7] = arr[0][42]\n",
    "    output[8][6] = arr[0][67]\n",
    "    output[8][5] = arr[0][96]\n",
    "    output[8][4] = arr[0][128]\n",
    "    output[8][3] = arr[0][156]\n",
    "    output[8][2] = arr[0][196]\n",
    "    output[8][1] = arr[0][231]\n",
    "    \n",
    "    #1\n",
    "    output[9][19] = arr[0][243]\n",
    "    output[9][18] = arr[0][208]\n",
    "    output[9][17] = arr[0][170]\n",
    "    output[9][16] = arr[0][143]\n",
    "    output[9][15] = arr[0][111]\n",
    "    output[9][14] = arr[0][80]\n",
    "    output[9][13] = arr[0][53]\n",
    "    output[9][12] = arr[0][30]\n",
    "    output[9][11] = arr[0][13]\n",
    "    output[9][10] = arr[0][0]\n",
    "    output[9][9] = arr[0][9]\n",
    "    output[9][8] = arr[0][24]\n",
    "    output[9][7] = arr[0][43]\n",
    "    output[9][6] = arr[0][68]\n",
    "    output[9][5] = arr[0][97]\n",
    "    output[9][4] = arr[0][129]\n",
    "    output[9][3] = arr[0][157]\n",
    "    output[9][2] = arr[0][197]\n",
    "    output[9][1] = arr[0][232]\n",
    "    \n",
    "    #12\n",
    "    output[10][18] = arr[0][225]\n",
    "    output[10][17] = arr[0][191]\n",
    "    output[10][16] = arr[0][142]\n",
    "    output[10][15] = arr[0][110]\n",
    "    output[10][14] = arr[0][79]\n",
    "    output[10][13] = arr[0][52]\n",
    "    output[10][12] = arr[0][29]\n",
    "    output[10][11] = arr[0][12]\n",
    "    output[10][10] = arr[0][11]\n",
    "    output[10][9] = arr[0][10]\n",
    "    output[10][8] = arr[0][25]\n",
    "    output[10][7] = arr[0][44]\n",
    "    output[10][6] = arr[0][69]\n",
    "    output[10][5] = arr[0][98]\n",
    "    output[10][4] = arr[0][130]\n",
    "    output[10][3] = arr[0][179]\n",
    "    output[10][2] = arr[0][213]\n",
    "    \n",
    "    #28\n",
    "    output[11][16] = arr[0][169]\n",
    "    output[11][15] = arr[0][141]\n",
    "    output[11][14] = arr[0][109]\n",
    "    output[11][13] = arr[0][78]\n",
    "    output[11][12] = arr[0][51]\n",
    "    output[11][11] = arr[0][28]\n",
    "    output[11][10] = arr[0][27]\n",
    "    output[11][9] = arr[0][26]\n",
    "    output[11][8] = arr[0][45]\n",
    "    output[11][7] = arr[0][70]\n",
    "    output[11][6] = arr[0][99]\n",
    "    output[11][5] = arr[0][131]\n",
    "    output[11][4] = arr[0][158]\n",
    "    \n",
    "    #49\n",
    "    output[12][17] = arr[0][190]\n",
    "    output[12][16] = arr[0][168]\n",
    "    output[12][15] = arr[0][140]\n",
    "    output[12][14] = arr[0][108]\n",
    "    output[12][13] = arr[0][77]\n",
    "    output[12][12] = arr[0][50]\n",
    "    output[12][11] = arr[0][49]\n",
    "    output[12][10] = arr[0][48]\n",
    "    output[12][9] = arr[0][47]\n",
    "    output[12][8] = arr[0][46]\n",
    "    output[12][7] = arr[0][71]\n",
    "    output[12][6] = arr[0][100]\n",
    "    output[12][5] = arr[0][132]\n",
    "    output[12][4] = arr[0][159]\n",
    "    output[12][3] = arr[0][180]\n",
    "\n",
    "    \n",
    "    #75\n",
    "    output[13][18] = arr[0][224]\n",
    "    output[13][17] = arr[0][207]\n",
    "    output[13][16] = arr[0][189]\n",
    "    output[13][15] = arr[0][167]\n",
    "    output[13][14] = arr[0][139]\n",
    "    output[13][13] = arr[0][107]\n",
    "    output[13][12] = arr[0][76]\n",
    "    output[13][11] = arr[0][75]\n",
    "    output[13][10] = arr[0][74]\n",
    "    output[13][9] = arr[0][73]\n",
    "    output[13][8] = arr[0][72]\n",
    "    output[13][7] = arr[0][101]\n",
    "    output[13][6] = arr[0][133]\n",
    "    output[13][5] = arr[0][160]\n",
    "    output[13][4] = arr[0][181]\n",
    "    output[13][3] = arr[0][198]\n",
    "    output[13][2] = arr[0][214]\n",
    "    \n",
    "    #105\n",
    "    output[14][18] = arr[0][242]\n",
    "    output[14][17] = arr[0][223]\n",
    "    output[14][16] = arr[0][206]\n",
    "    output[14][15] = arr[0][188]\n",
    "    output[14][14] = arr[0][166]\n",
    "    output[14][13] = arr[0][138]\n",
    "    output[14][12] = arr[0][106]\n",
    "    output[14][11] = arr[0][105]\n",
    "    output[14][10] = arr[0][104]\n",
    "    output[14][9] = arr[0][103]\n",
    "    output[14][8] = arr[0][102]\n",
    "    output[14][7] = arr[0][134]\n",
    "    output[14][6] = arr[0][161]\n",
    "    output[14][5] = arr[0][182]\n",
    "    output[14][4] = arr[0][199]\n",
    "    output[14][3] = arr[0][215]\n",
    "    output[14][2] = arr[0][233]\n",
    "    \n",
    "    \n",
    "    #137\n",
    "    output[15][16] = arr[0][241]\n",
    "    output[15][15] = arr[0][222]\n",
    "    output[15][14] = arr[0][205]\n",
    "    output[15][13] = arr[0][187]\n",
    "    output[15][12] = arr[0][165]\n",
    "    output[15][11] = arr[0][137]\n",
    "    output[15][10] = arr[0][136]\n",
    "    output[15][9] = arr[0][135]\n",
    "    output[15][8] = arr[0][162]\n",
    "    output[15][7] = arr[0][183]\n",
    "    output[15][6] = arr[0][200]\n",
    "    output[15][5] = arr[0][216]\n",
    "    output[15][4] = arr[0][234]\n",
    "    \n",
    "    \n",
    "    #mix\n",
    "    output[16][15] = arr[0][240]\n",
    "    output[16][14] = arr[0][221]\n",
    "    output[16][13] = arr[0][204]\n",
    "    output[16][12] = arr[0][186]\n",
    "    output[16][11] = arr[0][164]\n",
    "    output[16][10] = arr[0][163]\n",
    "    output[16][9] = arr[0][184]\n",
    "    output[16][8] = arr[0][201]\n",
    "    output[16][7] = arr[0][217]\n",
    "    output[16][6] = arr[0][235]\n",
    "   \n",
    "    #186\n",
    "    output[17][12] = arr[0][220]\n",
    "    output[17][11] = arr[0][203]\n",
    "    output[17][10] = arr[0][185]\n",
    "    output[17][9] = arr[0][202]\n",
    "    output[17][8] = arr[0][218]\n",
    "   \n",
    "    #220\n",
    "    output[18][11] = arr[0][239]\n",
    "    output[18][10] = arr[0][219]\n",
    "    output[18][9] = arr[0][236]\n",
    "    \n",
    "    #mix\n",
    "    output[19][11] = arr[0][238]\n",
    "    output[19][10] = arr[0][237]\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "training_file_dir = \"Data/train\"\n",
    "all_train_files = [f for f in listdir(training_file_dir) if isfile(join(training_file_dir, f))]\n",
    "train_files_dirs = []\n",
    "for i in range(len(all_train_files)):\n",
    "    train_files_dirs.append(training_file_dir+'/'+all_train_files[i])\n",
    "rest_list, mem_list, math_list, motor_list = separate_list(train_files_dirs)\n",
    "train_files_dirs = order_arranging(rest_list, mem_list, math_list, motor_list)\n",
    "\n",
    "\n",
    "validation_file_dir = \"Data/validate\"\n",
    "all_validate_files = [f for f in listdir(validation_file_dir) if isfile(join(validation_file_dir, f))]\n",
    "validate_files_dirs = []\n",
    "for i in range(len(all_validate_files)):\n",
    "    validate_files_dirs.append(validation_file_dir+'/'+all_validate_files[i])\n",
    "rest_list, mem_list, math_list, motor_list = separate_list(validate_files_dirs)\n",
    "validate_files_dirs = order_arranging(rest_list, mem_list, math_list, motor_list)\n",
    "\n",
    "\n",
    "test_file_dir = \"Data/test\"\n",
    "all_test_files = [f for f in listdir(test_file_dir) if isfile(join(test_file_dir, f))]\n",
    "test_files_dirs = []\n",
    "for i in range(len(all_test_files)):\n",
    "    test_files_dirs.append(test_file_dir+'/'+all_test_files[i])\n",
    "rest_list, mem_list, math_list, motor_list = separate_list(test_files_dirs)\n",
    "test_files_dirs = order_arranging(rest_list, mem_list, math_list, motor_list)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
