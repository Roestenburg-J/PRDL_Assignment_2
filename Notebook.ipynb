{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras as kr\n",
    "import tensorflow as tf\n",
    "\n",
    "# Locally get vars to accommodate different environments\n",
    "try:\n",
    "    from env_vars import *  \n",
    "except:\n",
    "    print(\"No env_vars file found. Using default values. Make a \\\"env_vars.py\\\" file to change them.\")\n",
    "    intra_train_path = \"your path here\"\n",
    "    intra_test_path  = \"your path here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge all files into one dataset\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "def get_dataset_name(file_name_with_dir):\n",
    "    filename_without_dir = file_name_with_dir.split(\"/\")[-1]\n",
    "    temp = filename_without_dir.split(\"_\")[:-1]\n",
    "    datasetname = \"_\".join(temp)\n",
    "    return datasetname\n",
    "\n",
    "def get_label(filename):\n",
    "    if 'rest' in filename:\n",
    "        label = 0\n",
    "    elif 'math' in filename:\n",
    "        label = 1\n",
    "    elif 'memory' in filename:\n",
    "        label = 2\n",
    "    elif 'motor' in filename:\n",
    "        label = 3\n",
    "    return label\n",
    "\n",
    "def get_all_matrices(dir_path):\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(dir_path):\n",
    "        if filename.endswith(\".h5\"):\n",
    "            filename_path = dir_path + filename\n",
    "            with h5py.File(filename_path, 'r') as f:\n",
    "                dataset_name = get_dataset_name(filename_path)\n",
    "                label = get_label(filename)\n",
    "                matrix = f.get(dataset_name)[()]\n",
    "                dataset.append(matrix)\n",
    "                labels.append(label)\n",
    "              \n",
    "\n",
    "    return dataset, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.74740712 0.74772078 0.74885491 ... 0.73938871 0.73718579 0.73627912]\n",
      "  [0.40877146 0.40968423 0.4112499  ... 0.38505688 0.38551707 0.38728114]\n",
      "  [0.7009247  0.70096807 0.70151863 ... 0.71032065 0.70880436 0.70774143]\n",
      "  ...\n",
      "  [0.774183   0.77379063 0.7753733  ... 0.77593174 0.77662985 0.77914528]\n",
      "  [0.43186421 0.43222923 0.43327232 ... 0.41756787 0.41650519 0.41809432]\n",
      "  [0.54323608 0.54463592 0.54744894 ... 0.53403411 0.53460681 0.53576734]]\n",
      "\n",
      " [[0.94303924 0.9417854  0.93858264 ... 0.93988644 0.94183427 0.94208164]\n",
      "  [0.90332562 0.90380797 0.90315913 ... 0.89520783 0.89568684 0.89679132]\n",
      "  [0.96081808 0.96108471 0.95981947 ... 0.96375179 0.96319005 0.96258611]\n",
      "  ...\n",
      "  [0.95199669 0.95089824 0.9487248  ... 0.95780987 0.95791452 0.95732887]\n",
      "  [0.94888014 0.94871092 0.9471317  ... 0.94356784 0.94394967 0.94381337]\n",
      "  [0.89620272 0.89621608 0.89675431 ... 0.92034025 0.92086643 0.9212247 ]]\n",
      "\n",
      " [[0.96028099 0.96041334 0.96033721 ... 0.96569543 0.96579696 0.96538085]\n",
      "  [0.96302199 0.96303362 0.96304975 ... 0.97639539 0.9756768  0.97550314]\n",
      "  [0.95665365 0.95680321 0.95711608 ... 0.96303092 0.96343868 0.96394923]\n",
      "  ...\n",
      "  [0.96166635 0.96166187 0.96175405 ... 0.96922982 0.96879814 0.96842051]\n",
      "  [0.96949205 0.96972512 0.97008268 ... 0.97236872 0.97253419 0.97283735]\n",
      "  [0.98360509 0.98414646 0.98448376 ... 0.98427191 0.98394178 0.98388648]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.94553439 0.94412158 0.94176423 ... 0.9419008  0.94102926 0.94052014]\n",
      "  [0.9297528  0.93085619 0.93239729 ... 0.93839622 0.93631563 0.93439331]\n",
      "  [0.97015296 0.9691844  0.96829605 ... 0.96936027 0.96907998 0.96780735]\n",
      "  ...\n",
      "  [0.95178318 0.95074138 0.95087531 ... 0.95494775 0.95416179 0.95141125]\n",
      "  [0.95810528 0.956524   0.95599482 ... 0.96066808 0.95977145 0.95748328]\n",
      "  [0.90374372 0.90182014 0.90017942 ... 0.90304617 0.90359678 0.90364193]]\n",
      "\n",
      " [[0.90981029 0.9091524  0.90831862 ... 0.91721123 0.91945462 0.92055996]\n",
      "  [0.90525177 0.90681904 0.90783737 ... 0.91595164 0.91650847 0.91645819]\n",
      "  [0.93674189 0.93710504 0.93735486 ... 0.94299148 0.94422109 0.94461641]\n",
      "  ...\n",
      "  [0.9059492  0.90693244 0.90750755 ... 0.91908492 0.92053605 0.9212121 ]\n",
      "  [0.92116251 0.92204523 0.92216772 ... 0.92760098 0.92942733 0.9300787 ]\n",
      "  [0.89259219 0.89322911 0.8934427  ... 0.90902096 0.91039666 0.91120327]]\n",
      "\n",
      " [[0.93432193 0.93587945 0.93634047 ... 0.93866673 0.9365533  0.93775716]\n",
      "  [0.91591282 0.91516879 0.91535594 ... 0.903642   0.90008371 0.89955371]\n",
      "  [0.96015117 0.95829701 0.95764579 ... 0.95713609 0.95511349 0.95581737]\n",
      "  ...\n",
      "  [0.92972647 0.93092357 0.93225182 ... 0.95080232 0.94910742 0.94960046]\n",
      "  [0.94657944 0.94487602 0.94505244 ... 0.94562495 0.94308146 0.94380752]\n",
      "  [0.87243149 0.8723571  0.87157326 ... 0.89694152 0.89558774 0.89550477]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = get_all_matrices(intra_train_path)[0]\n",
    "# subjects x sensors x time\n",
    "dataset_X = np.stack(dataset) \n",
    "labels = get_all_matrices(intra_train_path)[1]\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_dataset_X = []\n",
    "\n",
    "for data in dataset_X:\n",
    "    scaler.fit(data)\n",
    "    scaled_data = scaler.transform(data)\n",
    "    scaled_dataset_X.append(scaled_data)\n",
    "\n",
    "trainX = np.stack(scaled_dataset_X)\n",
    "\n",
    "print(trainX)\n",
    "trainY = labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 248, 35624)\n"
     ]
    }
   ],
   "source": [
    "dataset = get_all_matrices(intra_test_path)[0]\n",
    "# subjects x sensors x time\n",
    "dataset_X = np.stack(dataset) \n",
    "labels = get_all_matrices(intra_test_path)[1]\n",
    "\n",
    "testX = dataset_X\n",
    "print(testX.shape)\n",
    "testY = labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdatasetname(file_name_with_dir):\n",
    "    filename_without_dir = file_name_with_dir.split(\"/\")[-1]\n",
    "    temp = filename_without_dir.split(\"_\")[:-1]\n",
    "    datasetname = \"_\".join(temp)\n",
    "    return datasetname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to open file (unable to open file: name = '/Users/stijn/Desktop/Final Project data/Intra/test/rest_105923_1.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m filename_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/stijn/Desktop/Final Project data/Intra/test/rest_105923_1.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m     dataset_name \u001b[38;5;241m=\u001b[39m getdatasetname(filename_path)\n\u001b[1;32m      4\u001b[0m     matrix \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mget(dataset_name)[()]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/h5py/_hl/files.py:562\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    553\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    554\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    555\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    556\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    557\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    558\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    559\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    560\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    561\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 562\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/h5py/_hl/files.py:235\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    234\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 235\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    237\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = '/Users/stijn/Desktop/Final Project data/Intra/test/rest_105923_1.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "with h5py.File(intra_train_path, \"r\") as f:\n",
    "    dataset_name = getdatasetname(intra_train_path)\n",
    "    matrix = f.get(dataset_name)[()]\n",
    "    print(type(matrix))\n",
    "    print(matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(248, 35624)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(matrix)\n",
    "scaled_data = scaler.transform(matrix)\n",
    "print(scaled_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downsampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.514257620452312\n"
     ]
    }
   ],
   "source": [
    "print(scaled_data.shape[1] / 2034)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1    2    3    6    9   18  113  226  339  678 1017]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the factors of the sample rate 2034\n",
    "import math\n",
    "\n",
    "factors = []\n",
    "for x in range(1, 2034):\n",
    "    if 2034 % x == 0:\n",
    "        factors.append(x)\n",
    "\n",
    "factors = np.array(factors)\n",
    "\n",
    "print(factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each one of the factors can be used as a subsample size for the current sample rate of 2034.\n",
    "# You can specify a freqency (n) for the data you want to keep.\n",
    "# Each factor has a limit for (n), after which the subsample would be the same size as the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2034. 1017.  678.  339.  226.  113.   18.    9.    6.    3.    2.]\n"
     ]
    }
   ],
   "source": [
    "min_sample_freqency = 2034 / factors\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "\n",
    "print(min_sample_freqency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new sample frequency can be specified using factors of the original frequency 2034\n",
    "# A frequency of 1017 would mean every second value is kept i.e., 2034/1017 = 2\n",
    "# The lower the factor, the less data will be kept from the sample e.g., 2034/113 = 18 (Every 18th value will be kept)\n",
    "\n",
    "\n",
    "def donwsample(dataset, frequency):\n",
    "    downsampled_dataset = []\n",
    "\n",
    "    for i in range(0, dataset.shape[2], 2034):\n",
    "        second = dataset[:, i : i + 2034]\n",
    "        subsample = []\n",
    "\n",
    "        for j in range(0, 2034, int(2034 / frequency)):\n",
    "            if j < second.shape[2]:\n",
    "                measurement = second[:, j]\n",
    "                subsample.append(measurement)\n",
    "\n",
    "        downsampled_dataset.extend(subsample)\n",
    "\n",
    "    return np.array(downsampled_dataset).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 248, 8906)\n",
      "[[0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainx = trainX[:, :, 0::4]\n",
    "trainy = kr.utils.to_categorical(trainY)\n",
    "\n",
    "testx = testX[:, :, 0::4]\n",
    "testy = kr.utils.to_categorical(testY)\n",
    "\n",
    "print(testx.shape)\n",
    "print(testy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mSequential()\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39madd(kr\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mLSTM(\u001b[38;5;241m248\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(trainx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],trainx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])))\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39madd(kr\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.5\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(kr.layers.LSTM(248, input_shape=(trainx.shape[1],trainx.shape[2])))\n",
    "model.add(kr.layers.Dropout(0.5))\n",
    "model.add(kr.layers.Dense(248, activation='relu'))\n",
    "model.add(kr.layers.Dense(trainy.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
    "verbose, epochs, batch_size = 0, 30, 64\n",
    "\n",
    "predictions = model.predict(testx)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "true_labels = np.argmax(testy, axis=1)\n",
    "model.fit(trainx, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "print(cm)\n",
    "_, accuracy = model.evaluate(testx, testy, batch_size=batch_size, verbose=0)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
