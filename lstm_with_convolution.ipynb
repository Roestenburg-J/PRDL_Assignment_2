{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "NTjtRYkxL447"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No env_vars file found. Using default values. Make a \"env_vars.py\" file to change them.\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Locally get vars to accommodate different environments\n",
        "try:\n",
        "    from env_vars import *\n",
        "except:\n",
        "    print(\n",
        "        'No env_vars file found. Using default values. Make a \"env_vars.py\" file to change them.'\n",
        "    )\n",
        "    intra_train_path = \"./data/Cross/train/\"\n",
        "    intra_test_path = \"./data/Cross/test1/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLJacqmIQHq3",
        "outputId": "c39fec2f-d9ed-45e6-cf0c-319f9d138161"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "\n",
        "# drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "5ltmaALlL449"
      },
      "outputs": [],
      "source": [
        "# Used to get the name of the dataset, and by extension, the label of the action being performed\n",
        "def getdatasetname(file_name_with_dir):\n",
        "    filename_without_dir = file_name_with_dir.split(\"/\")[-1]\n",
        "\n",
        "    temp = filename_without_dir.split(\"_\")[:-1]\n",
        "\n",
        "    datasetname = \"_\".join(temp)\n",
        "\n",
        "    return datasetname"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "rk_y7Ak6L449"
      },
      "outputs": [],
      "source": [
        "# Used to get the labels of data using filenames\n",
        "def get_label(filename):\n",
        "    if \"rest\" in filename:\n",
        "        label = 0\n",
        "\n",
        "    elif \"math\" in filename:\n",
        "        label = 1\n",
        "\n",
        "    elif \"memory\" in filename:\n",
        "        label = 2\n",
        "    elif \"motor\" in filename:\n",
        "        label = 3\n",
        "\n",
        "    return label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "CqLPp7xKL44_"
      },
      "outputs": [],
      "source": [
        "def get_all_matrices(dir_path):\n",
        "    dataset = []\n",
        "    labels = []\n",
        "    for filename in os.listdir(dir_path):\n",
        "        if filename.endswith(\".h5\"):\n",
        "            filename_path = dir_path + filename\n",
        "            with h5py.File(filename_path, \"r\") as f:\n",
        "                dataset_name = getdatasetname(filename_path)\n",
        "                label = get_label(filename)\n",
        "                matrix = f.get(dataset_name)[()]\n",
        "                dataset.append(matrix)\n",
        "                labels.append(label)\n",
        "\n",
        "    return dataset, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MinMax Scaling for sensor data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "x0KQT9KkL45A"
      },
      "outputs": [],
      "source": [
        "def scale(matrix):\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "    scaler.fit(matrix)\n",
        "\n",
        "    scaled_data = scaler.transform(matrix)\n",
        "\n",
        "    return scaled_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Downsampling of data, refer to file called \"Notebook.ipynb\" to see how it works\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "UbSawgmiL45A"
      },
      "outputs": [],
      "source": [
        "def downsample(dataset, frequency):\n",
        "    downsampled_dataset = []\n",
        "\n",
        "    for i in range(0, dataset.shape[1], 2034):\n",
        "        second = dataset[:, i : i + 2034]\n",
        "        subsample = []\n",
        "\n",
        "        for j in range(0, 2034, int(2034 / frequency)):\n",
        "            if j < second.shape[1]:\n",
        "                measurement = second[:, j]\n",
        "                subsample.append(measurement)\n",
        "\n",
        "        downsampled_dataset.extend(subsample)\n",
        "\n",
        "    return np.array(downsampled_dataset).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYd4BFYML45D"
      },
      "source": [
        "Model setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-L0I5MdL45E",
        "outputId": "dca14458-a292-486a-ee93-0eacd90dc6a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.15.0\n"
          ]
        }
      ],
      "source": [
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is where the archtiecture is specified. For now it is just a Recurrent Neural Network (RNN) with two layers with 128 parameters each i.e., not very sophisticated.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "rup5-IlsMh3E"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Dense(\n",
        "            128, activation=\"relu\"\n",
        "        ),  # The layer has 128 dense neurons using relu for activation\n",
        "        tf.keras.layers.Dense(\n",
        "            128, activation=\"relu\"\n",
        "        ),  # The layer has 128 dense neurons using relu for activation\n",
        "        tf.keras.layers.Dense(\n",
        "            4\n",
        "        ),  # The final layer has 4 neurons, each representing a class for the action being done\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "Ok4qL_jsNbao"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[\"accuracy\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "N6LJwavfYyf7"
      },
      "outputs": [],
      "source": [
        "def load_data(dir_path):\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    for filename in os.listdir(dir_path):\n",
        "        if filename.endswith(\".h5\"):\n",
        "            filename_path = os.path.join(dir_path, filename)\n",
        "            with h5py.File(filename_path, \"r\") as f:\n",
        "                dataset_name = getdatasetname(filename_path)\n",
        "                label = get_label(filename)\n",
        "                matrix = f.get(dataset_name)[()]\n",
        "\n",
        "                train_meg = downsample(matrix, 113)\n",
        "                train_meg = scale(train_meg)\n",
        "                # flattened_meg = np.array(\n",
        "                #     train_meg.flatten()\n",
        "                # )  # The data is flattened to change the meg data from shape 248 x frequency to a 1D array (result: a lot of inupt params)\n",
        "\n",
        "                data.append(train_meg)\n",
        "                labels.append(label)\n",
        "\n",
        "    return np.array(data), np.array(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All training data is loaded into memory and fitted to the model along with the training lables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, y_train = load_data(intra_train_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def select_every_eighth(arr1_3d_list, arr2_1d):\n",
        "    # Convert arr1_3d_list to a list of NumPy arrays\n",
        "    arr1_3d_np_list = [np.array(arr) for arr in arr1_3d_list]\n",
        "\n",
        "    # Select every 8th value from the first 3D array in the list\n",
        "    selected_values_arr1_list = [arr[:, 7::8, :] for arr in arr1_3d_np_list]\n",
        "\n",
        "    # Remove every 8th value from the first 3D array in the list\n",
        "    modified_arr1_3d_list = [\n",
        "        np.delete(arr, range(7, arr.shape[1], 8), axis=1) for arr in arr1_3d_np_list\n",
        "    ]\n",
        "\n",
        "    # Convert arr2_1d to a NumPy array\n",
        "    arr2_1d_np = np.array(arr2_1d)\n",
        "\n",
        "    # Select every 8th value from the second 1D array\n",
        "    selected_values_arr2 = arr2_1d_np[7::8]\n",
        "\n",
        "    # Remove every 8th value from the second 1D array\n",
        "    modified_arr2_1d = np.delete(arr2_1d_np, range(7, len(arr2_1d_np), 8))\n",
        "\n",
        "    return (\n",
        "        modified_arr1_3d_list,\n",
        "        modified_arr2_1d,\n",
        "        selected_values_arr1_list,\n",
        "        selected_values_arr2,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "too many indices for array: array is 2-dimensional, but 3 were indexed",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[139], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train, y_train, X_val, y_val \u001b[38;5;241m=\u001b[39m select_every_eighth(X_train, y_train)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_train\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_train\u001b[38;5;241m.\u001b[39mshape)\n",
            "Cell \u001b[1;32mIn[138], line 9\u001b[0m, in \u001b[0;36mselect_every_eighth\u001b[1;34m(arr1_3d_list, arr2_1d)\u001b[0m\n\u001b[0;32m      6\u001b[0m arr1_3d_np_list \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39marray(arr) \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arr1_3d_list]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Select every 8th value from the first 3D array in the list\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m selected_values_arr1_list \u001b[38;5;241m=\u001b[39m [arr[:, \u001b[38;5;241m7\u001b[39m::\u001b[38;5;241m8\u001b[39m, :] \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arr1_3d_np_list]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Remove every 8th value from the first 3D array in the list\u001b[39;00m\n\u001b[0;32m     12\u001b[0m modified_arr1_3d_list \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     13\u001b[0m     np\u001b[38;5;241m.\u001b[39mdelete(arr, \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m7\u001b[39m, arr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m8\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arr1_3d_np_list\n\u001b[0;32m     14\u001b[0m ]\n",
            "Cell \u001b[1;32mIn[138], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      6\u001b[0m arr1_3d_np_list \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39marray(arr) \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arr1_3d_list]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Select every 8th value from the first 3D array in the list\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m selected_values_arr1_list \u001b[38;5;241m=\u001b[39m [arr[:, \u001b[38;5;241m7\u001b[39m::\u001b[38;5;241m8\u001b[39m, :] \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arr1_3d_np_list]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Remove every 8th value from the first 3D array in the list\u001b[39;00m\n\u001b[0;32m     12\u001b[0m modified_arr1_3d_list \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     13\u001b[0m     np\u001b[38;5;241m.\u001b[39mdelete(arr, \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m7\u001b[39m, arr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m8\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arr1_3d_np_list\n\u001b[0;32m     14\u001b[0m ]\n",
            "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 2-dimensional, but 3 were indexed"
          ]
        }
      ],
      "source": [
        "X_train, y_train, X_val, y_val = select_every_eighth(X_train, y_train)\n",
        "\n",
        "print(X_train.shape)\n",
        "\n",
        "\n",
        "\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Conv1D(\n",
        "            32,\n",
        "            kernel_size=3,\n",
        "            activation=\"relu\",\n",
        "            input_shape=(X_train.shape[1], X_train.shape[2]),\n",
        "        ),\n",
        "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "        # tf.keras.layers.BatchNormalization(),\n",
        "        # tf.keras.layers.DepthwiseConv2D(kernel_size=(1, 1), depth_multiplier=3),\n",
        "        # tf.keras.layers.SeparableConv2D(3, (1, 16), use_bias=False, padding=\"same\"),\n",
        "        # tf.keras.layers.Dense(128),\n",
        "        tf.keras.layers.LSTM(\n",
        "            64, return_sequences=True\n",
        "        ),  # Adjust the number of LSTM units\n",
        "        tf.keras.layers.Flatten(),  # Flatten the output of LSTM to connect with Dense layer\n",
        "        tf.keras.layers.Dense(4),\n",
        "        tf.keras.layers.Activation(\"softmax\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdCUhc98SEVx",
        "outputId": "372136ee-d0f3-41e5-d120-db5a50bf0c85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "2/2 [==============================] - 5s 147ms/step - loss: 2.0648 - accuracy: 0.3281\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 0s 154ms/step - loss: 2.1743 - accuracy: 0.1562\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 0s 143ms/step - loss: 2.0856 - accuracy: 0.2500\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 0s 145ms/step - loss: 1.5965 - accuracy: 0.1250\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 0s 147ms/step - loss: 1.7312 - accuracy: 0.2656\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 0s 134ms/step - loss: 1.3664 - accuracy: 0.5000\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 0s 145ms/step - loss: 1.2613 - accuracy: 0.4219\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 0s 132ms/step - loss: 1.3287 - accuracy: 0.3594\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 0s 151ms/step - loss: 1.1703 - accuracy: 0.3594\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 0s 133ms/step - loss: 1.1616 - accuracy: 0.4688\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x2478f577890>"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X_train, y_train, epochs=10, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test, y_test = load_data(intra_test_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gM8GXJeTlxx",
        "outputId": "db17792a-fff6-4510-c422-9603695b0547"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 - 1s - loss: 1.1299 - accuracy: 0.5000 - 740ms/epoch - 740ms/step\n",
            "\n",
            "Test accuracy: 0.5\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
        "\n",
        "print(\"\\nTest accuracy:\", test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Janus\\AppData\\Local\\Temp\\ipykernel_21688\\2126362821.py:1: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  from kerastuner.tuners import RandomSearch\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'X_val' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[103], line 49\u001b[0m\n\u001b[0;32m     39\u001b[0m tuner \u001b[38;5;241m=\u001b[39m RandomSearch(\n\u001b[0;32m     40\u001b[0m     build_model,\n\u001b[0;32m     41\u001b[0m     objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m     project_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_tuning_project\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     46\u001b[0m )\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Perform the hyperparameter search\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m tuner\u001b[38;5;241m.\u001b[39msearch(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_val, y_val))\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Get the best hyperparameters\u001b[39;00m\n\u001b[0;32m     52\u001b[0m best_hps \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_best_trials(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mhyperparameters\n",
            "\u001b[1;31mNameError\u001b[0m: name 'X_val' is not defined"
          ]
        }
      ],
      "source": [
        "from kerastuner.tuners import RandomSearch\n",
        "from kerastuner.engine.hyperparameters import HyperParameters\n",
        "\n",
        "# Define the hyperparameter search space\n",
        "hyperparameters = HyperParameters()\n",
        "hyperparameters.Int(\"units\", min_value=32, max_value=256, step=32)\n",
        "hyperparameters.Float(\"dropout\", min_value=0.2, max_value=0.5, step=0.1)\n",
        "\n",
        "\n",
        "# Define the model-building function\n",
        "def build_model(hp):\n",
        "    model = tf.keras.Sequential(\n",
        "        [\n",
        "            tf.keras.layers.Conv1D(\n",
        "                32,\n",
        "                kernel_size=3,\n",
        "                activation=\"relu\",\n",
        "                input_shape=(X_train.shape[1], X_train.shape[2]),\n",
        "            ),\n",
        "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "            tf.keras.layers.LSTM(\n",
        "                hp.Int(\"units\", min_value=16, max_value=256, step=8),\n",
        "                return_sequences=True,\n",
        "            ),\n",
        "            tf.keras.layers.Flatten(),\n",
        "            tf.keras.layers.Dropout(\n",
        "                hp.Float(\"dropout\", min_value=0.2, max_value=0.5, step=0.1)\n",
        "            ),\n",
        "            tf.keras.layers.Dense(4, activation=\"softmax\"),\n",
        "        ]\n",
        "    )\n",
        "    model.compile(\n",
        "        optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "# Instantiate the tuner\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_trials=10,\n",
        "    executions_per_trial=2,\n",
        "    directory=\"my_tuner_directory\",\n",
        "    project_name=\"my_tuning_project\",\n",
        ")\n",
        "\n",
        "# Perform the hyperparameter search\n",
        "tuner.search(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_hps = tuner.oracle.get_best_trials(1)[0].hyperparameters\n",
        "print(f\"Best Hyperparameters: {best_hps}\")\n",
        "\n",
        "# Build the model with the best hyperparameters and train on the full dataset\n",
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "best_model.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data_LTSM(dir_path):\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    for filename in os.listdir(dir_path):\n",
        "        if filename.endswith(\".h5\"):\n",
        "            filename_path = os.path.join(dir_path, filename)\n",
        "            with h5py.File(filename_path, \"r\") as f:\n",
        "                dataset_name = getdatasetname(filename_path)\n",
        "                label = get_label(filename)\n",
        "                matrix = f.get(dataset_name)[()]\n",
        "\n",
        "                train_meg = downsample(matrix, 113)\n",
        "                train_meg = scale(train_meg)\n",
        "                data.append(train_meg)\n",
        "                labels.append(label)\n",
        "\n",
        "    return np.array(data), np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(16, 248, 1980)\n",
            "[[1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]]\n"
          ]
        }
      ],
      "source": [
        "trainx, trainY = load_data_LTSM(intra_train_path)\n",
        "\n",
        "trainy = tf.keras.utils.to_categorical(trainY)\n",
        "\n",
        "testx, testY = load_data_LTSM(intra_test_path)\n",
        "\n",
        "testy = tf.keras.utils.to_categorical(testY)\n",
        "\n",
        "print(testx.shape)\n",
        "print(testy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"c:\\Users\\Janus\\anaconda3\\envs\\tf\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Janus\\anaconda3\\envs\\tf\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Janus\\anaconda3\\envs\\tf\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Janus\\anaconda3\\envs\\tf\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\Janus\\anaconda3\\envs\\tf\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\Janus\\anaconda3\\envs\\tf\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_7\" is incompatible with the layer: expected shape=(None, 3959, 20, 21), found shape=(64, 248, 1980)\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[51], line 43\u001b[0m\n\u001b[0;32m     36\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m     37\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     38\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mlegacy\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m),\n\u001b[0;32m     39\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     40\u001b[0m )\n\u001b[0;32m     41\u001b[0m verbose, epochs, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m64\u001b[39m\n\u001b[1;32m---> 43\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(trainx, trainy, epochs\u001b[38;5;241m=\u001b[39mepochs, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[0;32m     45\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(testx)\n\u001b[0;32m     46\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\Janus\\anaconda3\\envs\\tf\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filepjvjad1q.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Janus\\anaconda3\\envs\\tf\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Janus\\anaconda3\\envs\\tf\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Janus\\anaconda3\\envs\\tf\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Janus\\anaconda3\\envs\\tf\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\Janus\\anaconda3\\envs\\tf\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\Janus\\anaconda3\\envs\\tf\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_7\" is incompatible with the layer: expected shape=(None, 3959, 20, 21), found shape=(64, 248, 1980)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "##### Add convolutional layers\n",
        "# Convolutional layers\n",
        "model.add(\n",
        "    tf.keras.layers.Conv2D(\n",
        "        113, (3, 3), activation=\"relu\", padding=\"same\", input_shape=(3959, 20, 21)\n",
        "    )\n",
        ")\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.DepthwiseConv2D(kernel_size=(1, 1), depth_multiplier=3))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "model.add(tf.keras.layers.Activation(\"relu\"))\n",
        "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# model.add(tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "# model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(tf.keras.layers.SeparableConv2D(3, (1, 16), use_bias=False, padding=\"same\"))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "# Reshape data to be compatible with LSTM layer\n",
        "model.add(\n",
        "    tf.keras.layers.Reshape(\n",
        "        (model.output_shape[1], model.output_shape[2] * model.output_shape[3])\n",
        "    )\n",
        ")\n",
        "#####\n",
        "\n",
        "model.add(tf.keras.layers.LSTM(248, input_shape=(trainx.shape[1], trainx.shape[2])))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(248, activation=\"relu\"))\n",
        "model.add(tf.keras.layers.Activation(\"softmax\"))\n",
        "model.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.0001),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "verbose, epochs, batch_size = 0, 30, 64\n",
        "\n",
        "model.fit(trainx, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
        "\n",
        "predictions = model.predict(testx)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "true_labels = np.argmax(testy, axis=1)\n",
        "\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "print(cm)\n",
        "_, accuracy = model.evaluate(testx, testy, batch_size=batch_size, verbose=0)\n",
        "print(accuracy)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
